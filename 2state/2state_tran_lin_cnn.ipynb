{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8b1192d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1192d4",
        "outputId": "c21527fd-35b1-44a7-88ba-6ad8a4db79e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"Rostlab/prot_bert\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_CLASSES = 2  # Binary classification (0: no signal peptide, 1: signal peptide)\n",
        "MAX_LENGTH = 70 # max sequence has len 70 in unpartitioned dataset\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "WINDOW_SIZE = 35  # sliding window (odd because model predicts center residue)\n",
        "STRIDE = 1  # Step size for sliding window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "181f6793",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181f6793",
        "outputId": "86335777-4095-4365-9ac8-dbcaff2fd421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30, 1024, padding_idx=0)\n",
              "    (position_embeddings): Embedding(40000, 1024)\n",
              "    (token_type_embeddings): Embedding(2, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-29): 30 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, \"models/2state_tran_lin_cnn.pt\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6ce93383",
      "metadata": {
        "id": "6ce93383"
      },
      "outputs": [],
      "source": [
        "def get_protbert_window_embeddings(windows, batch_size=16):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(formatted), batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # outputs.last_hidden_state: (batch, seq_len, emb_dim)\n",
        "            # Remove [CLS] and [SEP] tokens if present\n",
        "            for j, seq in enumerate(batch_seqs):\n",
        "                seq_len = len(seq.replace(\" \", \"\"))\n",
        "                # Find where the actual window ends (excluding padding tokens)\n",
        "                emb = outputs.last_hidden_state[j, 1:seq_len+1, :].cpu().numpy()  # skip [CLS], take only window\n",
        "                all_embeddings.append(emb)\n",
        "\n",
        "    return np.stack(all_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "63ba15f2",
      "metadata": {
        "id": "63ba15f2"
      },
      "outputs": [],
      "source": [
        "def create_sliding_windows(sequence, labels, window_size, stride=1):\n",
        "    \"\"\"Create sliding windows from sequence and corresponding labels\"\"\"\n",
        "    windows = []\n",
        "    window_labels = []\n",
        "    positions = []\n",
        "\n",
        "    # Pad sequence for edge cases\n",
        "    pad_size = window_size // 2 # so starts classification after padding, at first real encoding\n",
        "    padded_seq = 'X' * pad_size + sequence + 'X' * pad_size\n",
        "    padded_labels = [0] * pad_size + labels + [0] * pad_size\n",
        "\n",
        "    # Create sliding windows\n",
        "    for i in range(0, len(sequence), stride):\n",
        "        start_idx = i\n",
        "        end_idx = i + window_size\n",
        "\n",
        "        if end_idx <= len(padded_seq):\n",
        "            window_seq = padded_seq[start_idx:end_idx]\n",
        "            # Label for the center position of the window\n",
        "            center_idx = start_idx + pad_size # residue to predict\n",
        "            if center_idx < len(padded_labels):\n",
        "                center_label = padded_labels[center_idx]\n",
        "\n",
        "                windows.append(window_seq)\n",
        "                window_labels.append(center_label)\n",
        "                positions.append(i)  # Original position in sequence\n",
        "\n",
        "    return windows, window_labels, positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "064304e2",
      "metadata": {
        "id": "064304e2"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(fasta_path):\n",
        "    \"\"\"Load FASTA data and preprocess for sliding window approach\"\"\"\n",
        "    records = []\n",
        "\n",
        "    with open(fasta_path, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                        records.append(current_record)\n",
        "\n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\n",
        "                    \"uniprot_ac\": uniprot_ac,\n",
        "                    \"kingdom\": kingdom,\n",
        "                    \"type\": type_,\n",
        "                    \"sequence\": None,\n",
        "                    \"label\": None\n",
        "                }\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "\n",
        "        # Add last record\n",
        "        if current_record is not None:\n",
        "            if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                records.append(current_record)\n",
        "\n",
        "    print(f\"Total records loaded: {len(records)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_raw = pd.DataFrame(records)\n",
        "\n",
        "    # Filter out sequences with 'P' in labels (if needed)\n",
        "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
        "\n",
        "    # Map signal peptide types to binary classification\n",
        "    df[\"has_signal_peptide\"] = df[\"type\"].map({\n",
        "        \"NO_SP\": 0,\n",
        "        \"LIPO\": 1,\n",
        "        \"SP\": 1,\n",
        "        \"TAT\": 1,\n",
        "        \"TATLIPO\": 1\n",
        "    })\n",
        "\n",
        "    # Balance the dataset at sequence level first\n",
        "    df_majority = df[df[\"has_signal_peptide\"] == 0]\n",
        "    df_minority = df[df[\"has_signal_peptide\"] == 1]\n",
        "\n",
        "    if not df_minority.empty and not df_majority.empty:\n",
        "\n",
        "        n_samples = min(len(df_majority), 5000) # Limit samples to 5000 to prevent high ram usage\n",
        "        df_majority_sampled = resample(\n",
        "            df_majority,\n",
        "            replace=False, # sample without replacement\n",
        "            n_samples=n_samples,\n",
        "            random_state=42\n",
        "        )\n",
        "        df_balanced = pd.concat([df_majority_sampled, df_minority]) # Include all minority samples\n",
        "    else:\n",
        "        df_balanced = df.copy()\n",
        "\n",
        "\n",
        "    # Convert residue-level labels to binary\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "\n",
        "    # Create sliding windows for all sequences\n",
        "    all_windows = []\n",
        "    all_labels = []\n",
        "    all_seq_ids = []\n",
        "\n",
        "    for idx, row in df_balanced.iterrows():\n",
        "        sequence = row[\"sequence\"]\n",
        "        label_string = row[\"label\"]\n",
        "\n",
        "        # Convert label string to binary array\n",
        "        residue_labels = [label_map.get(c, 0) for c in label_string]\n",
        "\n",
        "        # Skip sequences where label length doesn't match sequence length\n",
        "        if len(residue_labels) != len(sequence):\n",
        "            print(\"A sequence length is not equal to the label length\")\n",
        "            continue\n",
        "\n",
        "        # Create sliding windows for this sequence\n",
        "        windows, window_labels, positions = create_sliding_windows(\n",
        "            sequence, residue_labels, WINDOW_SIZE, STRIDE\n",
        "        )\n",
        "\n",
        "        all_windows.extend(windows)\n",
        "        all_labels.extend(window_labels)\n",
        "        all_seq_ids.extend([idx] * len(windows))\n",
        "\n",
        "    print(f\"Total windows created: {len(all_windows)}\")\n",
        "    print(f\"Signal peptide windows: {sum(all_labels)}\")\n",
        "    print(f\"Non-signal peptide windows: {len(all_labels) - sum(all_labels)}\")\n",
        "\n",
        "    return all_windows, all_labels, all_seq_ids, df_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "408cc6b7",
      "metadata": {
        "id": "408cc6b7"
      },
      "outputs": [],
      "source": [
        "class LazySlidingWindowDataset(Dataset):\n",
        "    def __init__(self, embeddings_path, labels_path, indices):\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.labels_path = labels_path\n",
        "        self.indices = indices # Indices corresponding to the split (train, val, or test)\n",
        "\n",
        "        # Load the full embeddings and labels once\n",
        "        self.all_embeddings = np.load(self.embeddings_path, mmap_mode='r') # Use mmap_mode to avoid loading everything into memory\n",
        "        self.all_labels = np.load(self.labels_path, mmap_mode='r')\n",
        "\n",
        "        # Ensure indices are within bounds (should be handled by splitting logic, but good practice)\n",
        "        if max(indices) >= len(self.all_labels) or min(indices) < 0:\n",
        "             raise ValueError(\"Indices are out of bounds for the loaded data.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the index in the original full dataset\n",
        "        original_idx = self.indices[idx]\n",
        "\n",
        "        # Load the specific embedding and label using the original index\n",
        "        # Slicing with numpy arrays loaded via mmap_mode='r' is efficient\n",
        "        embedding = self.all_embeddings[original_idx]\n",
        "        label = self.all_labels[original_idx]\n",
        "\n",
        "        return {\n",
        "            'window': torch.tensor(embedding, dtype=torch.float32),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c777e161",
      "metadata": {
        "id": "c777e161"
      },
      "outputs": [],
      "source": [
        "class CNNLSTMSignalPeptideClassifier(nn.Module):\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=2,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = num_aa\n",
        "\n",
        "        for out_channels in cnn_channels:\n",
        "            self.conv_layers.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # LSTM layers for sequential dependencies\n",
        "        # Input to LSTM: [batch_size, seq_len, features]\n",
        "        lstm_input_size = cnn_channels[-1]  # Last CNN output channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate LSTM output size\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Attention mechanism to focus on important positions\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lstm_output_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        classifier_layers = []\n",
        "        in_dim = lstm_output_size\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "        # Binary classification output\n",
        "        classifier_layers.append(nn.Linear(hidden_dim, 1))\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size, seq_len, num_features = x.size()\n",
        "\n",
        "        # need [batch_size, num_aa, window_size] for Conv1d\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "\n",
        "        # Apply CNN layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        # need [batch_size, seq_len, features] for LSTM\n",
        "        x = x.transpose(1, 2)  # [batch_size, window_size, cnn_channels[-1]]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # lstm_out: [batch_size, seq_len, lstm_hidden * directions]\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Weighted sum of LSTM outputs\n",
        "        attended_output = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        # attended_output: [batch_size, lstm_hidden * directions]\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(attended_output)\n",
        "        return logits.squeeze(-1)  # Remove last dimension\n",
        "\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifierV2(nn.Module):\n",
        "    \"\"\"Alternative version with different CNN-LSTM integration\"\"\"\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=1,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv1d(num_aa, cnn_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(cnn_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv1d(cnn_channels[1], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for sequential modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate dimensions\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Global pooling options\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "        cnn_features = self.cnn_backbone(x)\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        x = cnn_features.transpose(1, 2)  # [batch_size, window_size, features]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling over sequence dimension\n",
        "        lstm_out = lstm_out.transpose(1, 2)  # [batch_size, features, seq_len]\n",
        "        pooled = self.global_pool(lstm_out).squeeze(-1)  # [batch_size, features]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "76c2361d",
      "metadata": {
        "id": "76c2361d"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs, device,\n",
        "                        lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    \"\"\"Enhanced training function with gradient clipping and better scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # More sophisticated learning rate scheduling\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=patience, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits = model(windows)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if train_batches == 0:\n",
        "            print(\"No successful training batches!\")\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                windows = batch['window'].to(device)\n",
        "                labels = batch['label'].to(device).float()\n",
        "\n",
        "                try:\n",
        "                    logits = model(windows)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    val_correct += (predictions == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_batches == 0:\n",
        "            print(\"No successful validation batches!\")\n",
        "            break\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping and best model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience * 2:  # More patience for complex model\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6713138b",
      "metadata": {
        "id": "6713138b"
      },
      "outputs": [],
      "source": [
        "# compute percentage of false predicted labels\n",
        "def sequence_level_accuracy(labels, predictions):\n",
        "    \"\"\"Compute the accuracy of individual window predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = len(labels) # Total number of windows\n",
        "\n",
        "    # Ensure labels and predictions have the same length\n",
        "    if len(labels) != len(predictions):\n",
        "        print(\"Warning: Length of labels and predictions do not match.\")\n",
        "        # Adjust total to the minimum length if lengths differ\n",
        "        total = min(len(labels), len(predictions))\n",
        "        labels = labels[:total]\n",
        "        predictions = predictions[:total]\n",
        "\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Now comparing individual predictions and labels\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0cfdfb0f",
      "metadata": {
        "id": "0cfdfb0f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the sliding window model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(windows)\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['No Signal', 'Signal']))\n",
        "\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    seq_acc = sequence_level_accuracy(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sequence-level Accuracy: {seq_acc:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def predict_sequence(model, sequence, window_size, device, threshold=0.5):\n",
        "    \"\"\"Predict signal peptide positions for a full sequence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy labels (we don't need them for prediction)\n",
        "    dummy_labels = [0] * len(sequence)\n",
        "\n",
        "    # Create sliding windows\n",
        "    windows, _, positions = create_sliding_windows(sequence, dummy_labels, window_size, stride=1)\n",
        "\n",
        "    # Encode windows\n",
        "    encoded_windows = get_protbert_window_embeddings(windows)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_window in encoded_windows:\n",
        "            window_tensor = torch.tensor(encoded_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            logit = model(window_tensor)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            pred = int(prob > threshold)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return predictions, probabilities, positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8ad5489",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8ad5489",
        "outputId": "876559aa-d45b-4745-b619-cadf70ade227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records loaded: 25693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-3961548838.py:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"has_signal_peptide\"] = df[\"type\"].map({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total windows created: 806011\n",
            "Signal peptide windows: 153863\n",
            "Non-signal peptide windows: 652148\n",
            "Encoding all windows...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|â–Š         | 4294/50376 [41:26<2883:14:09, 225.24s/it]"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data with sliding windows\n",
        "# This will create windows and labels but NOT encode them yet\n",
        "windows, labels, seq_ids, df_balanced = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "\n",
        "\n",
        "# --- Step 1: Pre-encode and Save Embeddings ---\n",
        "print(\"Encoding all windows...\")\n",
        "# Process windows in batches to manage memory during encoding\n",
        "all_embeddings = get_protbert_window_embeddings(windows)\n",
        "\n",
        "# Save embeddings and labels to disk\n",
        "embeddings_path = os.path.join(DRIVE_PATH, \"all_window_embeddings.npy\")\n",
        "labels_path = os.path.join(DRIVE_PATH, \"all_window_labels.npy\")\n",
        "df_balanced_path = os.path.join(DRIVE_PATH, \"df_balanced.csv\") # Save the balanced dataframe for later use if needed\n",
        "\n",
        "np.save(embeddings_path, all_embeddings)\n",
        "np.save(labels_path, np.array(labels))\n",
        "df_balanced.to_csv(df_balanced_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "print(f\"Labels saved to {labels_path}\")\n",
        "print(f\"Balanced DataFrame saved to {df_balanced_path}\")\n",
        "\n",
        "# --- Step 2 & 3: Create Dataset instances using LazySlidingWindowDataset and Update Training/Evaluation ---\n",
        "\n",
        "# Split indices based on unique sequence IDs to avoid data leakage\n",
        "unique_seq_ids = list(df_balanced.index.unique()) # Use index from df_balanced\n",
        "train_seq_ids, temp_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "val_seq_ids, test_seq_ids = train_test_split(temp_seq_ids, test_size=0.5, random_state=42) # 0.5 of 0.2 = 0.1 test size\n",
        "\n",
        "# Get indices corresponding to each split based on the original df_balanced index\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "print(f\"\\nTrain windows (indices): {len(train_indices)}\")\n",
        "print(f\"Validation windows (indices): {len(val_indices)}\")\n",
        "print(f\"Test windows (indices): {len(test_indices)}\")\n",
        "\n",
        "# Create datasets and loaders using the saved files and indices\n",
        "train_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, train_indices)\n",
        "val_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, val_indices)\n",
        "test_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, all_embeddings.shape[-1], hidden_dim=128, num_layers=2 # Use embedding dim from the saved file\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e0113a",
      "metadata": {
        "id": "75e0113a"
      },
      "outputs": [],
      "source": [
        "# Load best model for evaluation\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nFinal Evaluation:\")\n",
        "predictions, labels_true, probabilities = evaluate_model(model, test_loader, DEVICE)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Curves')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(probabilities, bins=50, alpha=0.7, label='All Predictions')\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Prediction Probability Distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Predict on a sample sequence\n",
        "sample_sequence = \"MKKTAIAIAVALAGFATVAQAAPKDNTWYTGAKLGHLQGPVRGVNPTTNAASMKNFTNDIKKEDTSFVTLDAAQ\"\n",
        "print(f\"\\nExample prediction for sequence: {sample_sequence}\")\n",
        "preds, probs, pos = predict_sequence(model, sample_sequence, WINDOW_SIZE, DEVICE)\n",
        "\n",
        "print(\"Position\\tAA\\tProbability\\tPrediction\")\n",
        "for i, (pred, prob, position) in enumerate(zip(preds, probs, pos)):\n",
        "    aa = sample_sequence[position] if position < len(sample_sequence) else 'X'\n",
        "    print(f\"{position:3d}\\t{aa}\\t{prob:.3f}\\t\\t{'Signal' if pred else 'No Signal'}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), MODEL_PATH)"
      ],
      "metadata": {
        "id": "v-MEP30AvNtE"
      },
      "id": "v-MEP30AvNtE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ffd5d9"
      },
      "source": [
        "# Task\n",
        "Explain why the provided training loop output shows only 3 batches at a time and suggest code modifications to address potential high RAM usage, including changes to the dataset class and embedding generation."
      ],
      "id": "22ffd5d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "977e22ec"
      },
      "source": [
        "## Modify `get protbert window embeddings`\n",
        "\n",
        "### Subtask:\n",
        "Change the `get_protbert_window_embeddings` function to process windows in smaller batches and save them directly to a NumPy file using `mmap_mode='w+'` or process and save in chunks.\n"
      ],
      "id": "977e22ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "585895ab"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `get_protbert_window_embeddings` function to handle potential memory issues by allowing direct saving to a memory-mapped file. This involves adding an `output_path` parameter, initializing a memory-mapped array, and writing batches directly to it. I will modify the existing function definition and logic within the `code_block`.\n",
        "\n"
      ],
      "id": "585895ab"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "293b779c"
      },
      "source": [
        "def get_protbert_window_embeddings(windows, batch_size=16, output_path=None, embedding_dim=1024):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    If output_path is provided, saves embeddings to a memory-mapped file.\n",
        "    Otherwise, returns a concatenated NumPy array.\n",
        "    \"\"\"\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "    num_windows = len(formatted)\n",
        "    window_size = len(windows[0]) # Assuming all windows have the same size after padding/truncation\n",
        "\n",
        "    if output_path:\n",
        "        # Initialize memory-mapped array\n",
        "        # Need to estimate the exact sequence length after tokenization and potential padding/truncation\n",
        "        # A safer approach is to determine the max length after tokenization or use the known MAX_LENGTH\n",
        "        # Let's use MAX_LENGTH here, assuming it's the effective sequence length after tokenization and padding\n",
        "        print(f\"Initializing memory-mapped file at {output_path} with shape ({num_windows}, {MAX_LENGTH}, {embedding_dim})\")\n",
        "        all_embeddings_mmap = np.memmap(output_path, dtype='float32', mode='w+', shape=(num_windows, MAX_LENGTH, embedding_dim))\n",
        "\n",
        "    all_embeddings_list = [] # Keep this for the case where output_path is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, num_windows, batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            batch_embeddings = outputs.last_hidden_state.cpu().numpy() # (batch, seq_len, emb_dim)\n",
        "\n",
        "            # prot_bert adds [CLS] and [SEP]. We need to slice to get the actual window embeddings\n",
        "            # The actual sequence length before padding can vary within a batch due to truncation\n",
        "            # and the original un-padded window length.\n",
        "            # However, for consistent window embeddings of size MAX_LENGTH, we can just slice\n",
        "            # from index 1 up to MAX_LENGTH + 1 (to exclude CLS and include MAX_LENGTH tokens).\n",
        "            # If padding is present, the embeddings for padding tokens will be there but won't affect\n",
        "            # the actual sequence residues.\n",
        "            processed_batch_embeddings = batch_embeddings[:, 1:MAX_LENGTH+1, :] # Remove [CLS] token embedding\n",
        "\n",
        "            if output_path:\n",
        "                # Write directly to the memory-mapped array\n",
        "                end_idx = min(i + batch_size, num_windows)\n",
        "                all_embeddings_mmap[i:end_idx] = processed_batch_embeddings[:end_idx-i] # Handle the last batch size\n",
        "\n",
        "            else:\n",
        "                # Append to the list if not saving to file\n",
        "                for emb in processed_batch_embeddings:\n",
        "                    all_embeddings_list.append(emb)\n",
        "\n",
        "    if output_path:\n",
        "        # Ensure all changes are written to disk\n",
        "        all_embeddings_mmap.flush()\n",
        "        # The memory-mapped file will be returned. It behaves like a numpy array.\n",
        "        return all_embeddings_mmap\n",
        "    else:\n",
        "        # Return concatenated array\n",
        "        return np.stack(all_embeddings_list)\n"
      ],
      "id": "293b779c",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48be7e92"
      },
      "source": [
        "## Update main execution cell\n",
        "\n",
        "### Subtask:\n",
        "Adjust the main cell to use the modified `get_protbert_window_embeddings` function and handle the creation of the `.npy` file incrementally.\n"
      ],
      "id": "48be7e92"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17561241"
      },
      "source": [
        "**Reasoning**:\n",
        "Adjust the main execution cell to use the modified `get_protbert_window_embeddings` function with the output path and remove the unnecessary `np.save` call for embeddings.\n",
        "\n"
      ],
      "id": "17561241"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "b1960601",
        "outputId": "72e07f2f-b233-403d-fe75-92ac6ec9369a"
      },
      "source": [
        "# Load and preprocess data with sliding windows\n",
        "# This will create windows and labels but NOT encode them yet\n",
        "windows, labels, seq_ids, df_balanced = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "# --- Step 1: Pre-encode and Save Embeddings ---\n",
        "print(\"Encoding all windows...\")\n",
        "# Process windows in batches and save directly to disk\n",
        "embeddings_path = os.path.join(DRIVE_PATH, \"all_window_embeddings.npy\")\n",
        "labels_path = os.path.join(DRIVE_PATH, \"all_window_labels.npy\")\n",
        "df_balanced_path = os.path.join(DRIVE_PATH, \"df_balanced.csv\") # Save the balanced dataframe for later use if needed\n",
        "\n",
        "# Assuming the first window's embedding size will be consistent\n",
        "dummy_encoding = get_protbert_window_embeddings([windows[0]])\n",
        "embedding_dim = dummy_encoding.shape[-1]\n",
        "del dummy_encoding # Free up memory\n",
        "\n",
        "# Use the modified function to save embeddings incrementally\n",
        "all_embeddings = get_protbert_window_embeddings(\n",
        "    windows,\n",
        "    batch_size=BATCH_SIZE, # Use same batch size as for training/inference\n",
        "    output_path=embeddings_path,\n",
        "    embedding_dim=embedding_dim\n",
        ")\n",
        "\n",
        "# Save labels and balanced dataframe\n",
        "np.save(labels_path, np.array(labels))\n",
        "df_balanced.to_csv(df_balanced_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "print(f\"Labels saved to {labels_path}\")\n",
        "print(f\"Balanced DataFrame saved to {df_balanced_path}\")\n",
        "\n",
        "# --- Step 2 & 3: Create Dataset instances using LazySlidingWindowDataset and Update Training/Evaluation ---\n",
        "\n",
        "# Split indices based on unique sequence IDs to avoid data leakage\n",
        "unique_seq_ids = list(df_balanced.index.unique()) # Use index from df_balanced\n",
        "train_seq_ids, temp_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "val_seq_ids, test_seq_ids = train_test_split(temp_seq_ids, test_size=0.5, random_state=42) # 0.5 of 0.2 = 0.1 test size\n",
        "\n",
        "# Get indices corresponding to each split based on the original df_balanced index\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "print(f\"\\nTrain windows (indices): {len(train_indices)}\")\n",
        "print(f\"Validation windows (indices): {len(val_indices)}\")\n",
        "print(f\"Test windows (indices): {len(test_indices)}\")\n",
        "\n",
        "# Create datasets and loaders using the saved files and indices\n",
        "train_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, train_indices)\n",
        "val_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, val_indices)\n",
        "test_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "# Use the embedding dimension determined from the dummy encoding\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, embedding_dim, hidden_dim=128, num_layers=2\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)"
      ],
      "id": "b1960601",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_and_preprocess_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-4285911.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and preprocess data with sliding windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This will create windows and labels but NOT encode them yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFASTA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- Step 1: Pre-encode and Save Embeddings ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_and_preprocess_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17a06249"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `load_and_preprocess_data` function was not executed. Re-execute the cell containing this function definition and the subsequent cells that depend on it.\n",
        "\n"
      ],
      "id": "17a06249"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "os6gtcB0-0q9",
        "outputId": "95a423cb-6b33-438b-a3e1-e17c234199e4"
      },
      "source": [
        "def load_and_preprocess_data(fasta_path):\n",
        "    \"\"\"Load FASTA data and preprocess for sliding window approach\"\"\"\n",
        "    records = []\n",
        "\n",
        "    with open(fasta_path, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                        records.append(current_record)\n",
        "\n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\n",
        "                    \"uniprot_ac\": uniprot_ac,\n",
        "                    \"kingdom\": kingdom,\n",
        "                    \"type\": type_,\n",
        "                    \"sequence\": None,\n",
        "                    \"label\": None\n",
        "                }\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "\n",
        "        # Add last record\n",
        "        if current_record is not None:\n",
        "            if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                records.append(current_record)\n",
        "\n",
        "    print(f\"Total records loaded: {len(records)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_raw = pd.DataFrame(records)\n",
        "\n",
        "    # Filter out sequences with 'P' in labels (if needed)\n",
        "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
        "\n",
        "    # Map signal peptide types to binary classification\n",
        "    df[\"has_signal_peptide\"] = df[\"type\"].map({\n",
        "        \"NO_SP\": 0,\n",
        "        \"LIPO\": 1,\n",
        "        \"SP\": 1,\n",
        "        \"TAT\": 1,\n",
        "        \"TATLIPO\": 1\n",
        "    })\n",
        "\n",
        "    # Balance the dataset at sequence level first\n",
        "    df_majority = df[df[\"has_signal_peptide\"] == 0]\n",
        "    df_minority = df[df[\"has_signal_peptide\"] == 1]\n",
        "\n",
        "    if not df_minority.empty and not df_majority.empty:\n",
        "\n",
        "        n_samples = min(len(df_majority), 5000) # Limit samples to 5000 to prevent high ram usage\n",
        "        df_majority_sampled = resample(\n",
        "            df_majority,\n",
        "            replace=False, # sample without replacement\n",
        "            n_samples=n_samples,\n",
        "            random_state=42\n",
        "        )\n",
        "        df_balanced = pd.concat([df_majority_sampled, df_minority]) # Include all minority samples\n",
        "    else:\n",
        "        df_balanced = df.copy()\n",
        "\n",
        "\n",
        "    # Convert residue-level labels to binary\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "\n",
        "    # Create sliding windows for all sequences\n",
        "    all_windows = []\n",
        "    all_labels = []\n",
        "    all_seq_ids = []\n",
        "\n",
        "    for idx, row in df_balanced.iterrows():\n",
        "        sequence = row[\"sequence\"]\n",
        "        label_string = row[\"label\"]\n",
        "\n",
        "        # Convert label string to binary array\n",
        "        residue_labels = [label_map.get(c, 0) for c in label_string]\n",
        "\n",
        "        # Skip sequences where label length doesn't match sequence length\n",
        "        if len(residue_labels) != len(sequence):\n",
        "            print(\"A sequence length is not equal to the label length\")\n",
        "            continue\n",
        "\n",
        "        # Create sliding windows for this sequence\n",
        "        windows, window_labels, positions = create_sliding_windows(\n",
        "            sequence, residue_labels, WINDOW_SIZE, STRIDE\n",
        "        )\n",
        "\n",
        "        all_windows.extend(windows)\n",
        "        all_labels.extend(window_labels)\n",
        "        all_seq_ids.extend([idx] * len(windows))\n",
        "\n",
        "    print(f\"Total windows created: {len(all_windows)}\")\n",
        "    print(f\"Signal peptide windows: {sum(all_labels)}\")\n",
        "    print(f\"Non-signal peptide windows: {len(all_labels) - sum(all_labels)}\")\n",
        "\n",
        "    return all_windows, all_labels, all_seq_ids, df_balanced\n",
        "\n",
        "def get_protbert_window_embeddings(windows, batch_size=16, output_path=None, embedding_dim=1024):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    If output_path is provided, saves embeddings to a memory-mapped file.\n",
        "    Otherwise, returns a concatenated NumPy array.\n",
        "    \"\"\"\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "    num_windows = len(formatted)\n",
        "    window_size = len(windows[0]) # Assuming all windows have the same size after padding/truncation\n",
        "\n",
        "    if output_path:\n",
        "        # Initialize memory-mapped array\n",
        "        # Need to estimate the exact sequence length after tokenization and potential padding/truncation\n",
        "        # A safer approach is to determine the max length after tokenization or use the known MAX_LENGTH\n",
        "        # Let's use MAX_LENGTH here, assuming it's the effective sequence length after tokenization and padding\n",
        "        print(f\"Initializing memory-mapped file at {output_path} with shape ({num_windows}, {MAX_LENGTH}, {embedding_dim})\")\n",
        "        all_embeddings_mmap = np.memmap(output_path, dtype='float32', mode='w+', shape=(num_windows, MAX_LENGTH, embedding_dim))\n",
        "\n",
        "    all_embeddings_list = [] # Keep this for the case where output_path is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, num_windows, batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            batch_embeddings = outputs.last_hidden_state.cpu().numpy() # (batch, seq_len, emb_dim)\n",
        "\n",
        "            # prot_bert adds [CLS] and [SEP]. We need to slice to get the actual window embeddings\n",
        "            # The actual sequence length before padding can vary within a batch due to truncation\n",
        "            # and the original un-padded window length.\n",
        "            # However, for consistent window embeddings of size MAX_LENGTH, we can just slice\n",
        "            # from index 1 up to MAX_LENGTH + 1 (to exclude CLS and include MAX_LENGTH tokens).\n",
        "            # If padding is present, the embeddings for padding tokens will be there but won't affect\n",
        "            # the actual sequence residues.\n",
        "            processed_batch_embeddings = batch_embeddings[:, 1:MAX_LENGTH+1, :] # Remove [CLS] token embedding\n",
        "\n",
        "            if output_path:\n",
        "                # Write directly to the memory-mapped array\n",
        "                end_idx = min(i + batch_size, num_windows)\n",
        "                all_embeddings_mmap[i:end_idx] = processed_batch_embeddings[:end_idx-i] # Handle the last batch size\n",
        "\n",
        "            else:\n",
        "                # Append to the list if not saving to file\n",
        "                for emb in processed_batch_embeddings:\n",
        "                    all_embeddings_list.append(emb)\n",
        "\n",
        "    if output_path:\n",
        "        # Ensure all changes are written to disk\n",
        "        all_embeddings_mmap.flush()\n",
        "        # The memory-mapped file will be returned. It behaves like a numpy array.\n",
        "        return all_embeddings_mmap\n",
        "    else:\n",
        "        # Return concatenated array\n",
        "        return np.stack(all_embeddings_list)\n",
        "\n",
        "class LazySlidingWindowDataset(Dataset):\n",
        "    def __init__(self, embeddings_path, labels_path, indices):\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.labels_path = labels_path\n",
        "        self.indices = indices # Indices corresponding to the split (train, val, or test)\n",
        "\n",
        "        # Load the full embeddings and labels once\n",
        "        self.all_embeddings = np.load(self.embeddings_path, mmap_mode='r') # Use mmap_mode to avoid loading everything into memory\n",
        "        self.all_labels = np.load(self.labels_path, mmap_mode='r')\n",
        "\n",
        "        # Ensure indices are within bounds (should be handled by splitting logic, but good practice)\n",
        "        if max(indices) >= len(self.all_labels) or min(indices) < 0:\n",
        "             raise ValueError(\"Indices are out of bounds for the loaded data.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the index in the original full dataset\n",
        "        original_idx = self.indices[idx]\n",
        "\n",
        "        # Load the specific embedding and label using the original index\n",
        "        # Slicing with numpy arrays loaded via mmap_mode='r' is efficient\n",
        "        embedding = self.all_embeddings[original_idx]\n",
        "        label = self.all_labels[original_idx]\n",
        "\n",
        "        return {\n",
        "            'window': torch.tensor(embedding, dtype=torch.float32),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifier(nn.Module):\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=2,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = num_aa\n",
        "\n",
        "        for out_channels in cnn_channels:\n",
        "            self.conv_layers.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # LSTM layers for sequential dependencies\n",
        "        # Input to LSTM: [batch_size, seq_len, features]\n",
        "        lstm_input_size = cnn_channels[-1]  # Last CNN output channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate LSTM output size\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Attention mechanism to focus on important positions\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lstm_output_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        classifier_layers = []\n",
        "        in_dim = lstm_output_size\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "        # Binary classification output\n",
        "        classifier_layers.append(nn.Linear(hidden_dim, 1))\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size, seq_len, num_features = x.size()\n",
        "\n",
        "        # need [batch_size, num_aa, window_size] for Conv1d\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "\n",
        "        # Apply CNN layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        # need [batch_size, seq_len, features] for LSTM\n",
        "        x = x.transpose(1, 2)  # [batch_size, window_size, cnn_channels[-1]]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # lstm_out: [batch_size, seq_len, lstm_hidden * directions]\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Weighted sum of LSTM outputs\n",
        "        attended_output = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        # attended_output: [batch_size, lstm_hidden * directions]\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(attended_output)\n",
        "        return logits.squeeze(-1)  # Remove last dimension\n",
        "\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifierV2(nn.Module):\n",
        "    \"\"\"Alternative version with different CNN-LSTM integration\"\"\"\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=1,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv1d(num_aa, cnn_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(cnn_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv1d(cnn_channels[1], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for sequential modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate dimensions\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Global pooling options\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "        cnn_features = self.cnn_backbone(x)\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        x = cnn_features.transpose(1, 2)  # [batch_size, window_size, features]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling over sequence dimension\n",
        "        lstm_out = lstm_out.transpose(1, 2)  # [batch_size, features, seq_len]\n",
        "        pooled = self.global_pool(lstm_out).squeeze(-1)  # [batch_size, features]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs, device,\n",
        "                        lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    \"\"\"Enhanced training function with gradient clipping and better scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # More sophisticated learning rate scheduling\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=patience, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits = model(windows)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if train_batches == 0:\n",
        "            print(\"No successful training batches!\")\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                windows = batch['window'].to(device)\n",
        "                labels = batch['label'].to(device).float()\n",
        "\n",
        "                try:\n",
        "                    logits = model(windows)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    val_correct += (predictions == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_batches == 0:\n",
        "            print(\"No successful validation batches!\")\n",
        "            break\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping and best model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience * 2:  # More patience for complex model\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# compute percentage of false predicted labels\n",
        "def sequence_level_accuracy(labels, predictions):\n",
        "    \"\"\"Compute the accuracy of individual window predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = len(labels) # Total number of windows\n",
        "\n",
        "    # Ensure labels and predictions have the same length\n",
        "    if len(labels) != len(predictions):\n",
        "        print(\"Warning: Length of labels and predictions do not match.\")\n",
        "        # Adjust total to the minimum length if lengths differ\n",
        "        total = min(len(labels), len(predictions))\n",
        "        labels = labels[:total]\n",
        "        predictions = predictions[:total]\n",
        "\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Now comparing individual predictions and labels\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    return correct / total\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the sliding window model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(windows)\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['No Signal', 'Signal']))\n",
        "\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    seq_acc = sequence_level_accuracy(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sequence-level Accuracy: {seq_acc:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def predict_sequence(model, sequence, window_size, device, threshold=0.5):\n",
        "    \"\"\"Predict signal peptide positions for a full sequence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy labels (we don't need them for prediction)\n",
        "    dummy_labels = [0] * len(sequence)\n",
        "\n",
        "    # Create sliding windows\n",
        "    windows, _, positions = create_sliding_windows(sequence, dummy_labels, window_size, stride=1)\n",
        "\n",
        "    # Encode windows\n",
        "    encoded_windows = get_protbert_window_embeddings(windows)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_window in encoded_windows:\n",
        "            window_tensor = torch.tensor(encoded_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            logit = model(window_tensor)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            pred = int(prob > threshold)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return predictions, probabilities, positions\n",
        "\n",
        "# Load and preprocess data with sliding windows\n",
        "# This will create windows and labels but NOT encode them yet\n",
        "windows, labels, seq_ids, df_balanced = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "# --- Step 1: Pre-encode and Save Embeddings ---\n",
        "print(\"Encoding all windows...\")\n",
        "# Process windows in batches and save directly to disk\n",
        "embeddings_path = os.path.join(DRIVE_PATH, \"all_window_embeddings.npy\")\n",
        "labels_path = os.path.join(DRIVE_PATH, \"all_window_labels.npy\")\n",
        "df_balanced_path = os.path.join(DRIVE_PATH, \"df_balanced.csv\") # Save the balanced dataframe for later use if needed\n",
        "\n",
        "# Assuming the first window's embedding size will be consistent\n",
        "dummy_encoding = get_protbert_window_embeddings([windows[0]])\n",
        "embedding_dim = dummy_encoding.shape[-1]\n",
        "del dummy_encoding # Free up memory\n",
        "\n",
        "# Use the modified function to save embeddings incrementally\n",
        "all_embeddings = get_protbert_window_embeddings(\n",
        "    windows,\n",
        "    batch_size=BATCH_SIZE, # Use same batch size as for training/inference\n",
        "    output_path=embeddings_path,\n",
        "    embedding_dim=embedding_dim\n",
        ")\n",
        "\n",
        "# Save labels and balanced dataframe\n",
        "np.save(labels_path, np.array(labels))\n",
        "df_balanced.to_csv(df_balanced_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "print(f\"Labels saved to {labels_path}\")\n",
        "print(f\"Balanced DataFrame saved to {df_balanced_path}\")\n",
        "\n",
        "# --- Step 2 & 3: Create Dataset instances using LazySlidingWindowDataset and Update Training/Evaluation ---\n",
        "\n",
        "# Split indices based on unique sequence IDs to avoid data leakage\n",
        "unique_seq_ids = list(df_balanced.index.unique()) # Use index from df_balanced\n",
        "train_seq_ids, temp_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "val_seq_ids, test_seq_ids = train_test_split(temp_seq_ids, test_size=0.5, random_state=42) # 0.5 of 0.2 = 0.1 test size\n",
        "\n",
        "# Get indices corresponding to each split based on the original df_balanced index\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "print(f\"\\nTrain windows (indices): {len(train_indices)}\")\n",
        "print(f\"Validation windows (indices): {len(val_indices)}\")\n",
        "print(f\"Test windows (indices): {len(test_indices)}\")\n",
        "\n",
        "# Create datasets and loaders using the saved files and indices\n",
        "train_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, train_indices)\n",
        "val_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, val_indices)\n",
        "test_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "# Use the embedding dimension determined from the dummy encoding\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, embedding_dim, hidden_dim=128, num_layers=2\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)"
      ],
      "id": "os6gtcB0-0q9",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-1874519528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embeddings_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLazySlidingWindowDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef8f5f54"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `Dataset` and `DataLoader` were not defined in the current cell's execution context. Re-executing the cell that defines these imports and other necessary components before the failing cell should resolve this.\n",
        "\n"
      ],
      "id": "ef8f5f54"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "2HkL4M0u-65x",
        "outputId": "8d9870ed-5c35-42b3-9b64-987c9e64d9e5"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"Rostlab/prot_bert\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_CLASSES = 2  # Binary classification (0: no signal peptide, 1: signal peptide)\n",
        "MAX_LENGTH = 70 # max sequence has len 70 in unpartitioned dataset\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "WINDOW_SIZE = 35  # sliding window (odd because model predicts center residue)\n",
        "STRIDE = 1  # Step size for sliding window\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, \"models/2state_tran_lin_cnn.pt\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)\n",
        "\n",
        "def create_sliding_windows(sequence, labels, window_size, stride=1):\n",
        "    \"\"\"Create sliding windows from sequence and corresponding labels\"\"\"\n",
        "    windows = []\n",
        "    window_labels = []\n",
        "    positions = []\n",
        "\n",
        "    # Pad sequence for edge cases\n",
        "    pad_size = window_size // 2 # so starts classification after padding, at first real encoding\n",
        "    padded_seq = 'X' * pad_size + sequence + 'X' * pad_size\n",
        "    padded_labels = [0] * pad_size + labels + [0] * pad_size\n",
        "\n",
        "    # Create sliding windows\n",
        "    for i in range(0, len(sequence), stride):\n",
        "        start_idx = i\n",
        "        end_idx = i + window_size\n",
        "\n",
        "        if end_idx <= len(padded_seq):\n",
        "            window_seq = padded_seq[start_idx:end_idx]\n",
        "            # Label for the center position of the window\n",
        "            center_idx = start_idx + pad_size # residue to predict\n",
        "            if center_idx < len(padded_labels):\n",
        "                center_label = padded_labels[center_idx]\n",
        "\n",
        "                windows.append(window_seq)\n",
        "                window_labels.append(center_label)\n",
        "                positions.append(i)  # Original position in sequence\n",
        "\n",
        "    return windows, window_labels, positions\n",
        "\n",
        "def load_and_preprocess_data(fasta_path):\n",
        "    \"\"\"Load FASTA data and preprocess for sliding window approach\"\"\"\n",
        "    records = []\n",
        "\n",
        "    with open(fasta_path, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                        records.append(current_record)\n",
        "\n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\n",
        "                    \"uniprot_ac\": uniprot_ac,\n",
        "                    \"kingdom\": kingdom,\n",
        "                    \"type\": type_,\n",
        "                    \"sequence\": None,\n",
        "                    \"label\": None\n",
        "                }\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "\n",
        "        # Add last record\n",
        "        if current_record is not None:\n",
        "            if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                records.append(current_record)\n",
        "\n",
        "    print(f\"Total records loaded: {len(records)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_raw = pd.DataFrame(records)\n",
        "\n",
        "    # Filter out sequences with 'P' in labels (if needed)\n",
        "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
        "\n",
        "    # Map signal peptide types to binary classification\n",
        "    df[\"has_signal_peptide\"] = df[\"type\"].map({\n",
        "        \"NO_SP\": 0,\n",
        "        \"LIPO\": 1,\n",
        "        \"SP\": 1,\n",
        "        \"TAT\": 1,\n",
        "        \"TATLIPO\": 1\n",
        "    })\n",
        "\n",
        "    # Balance the dataset at sequence level first\n",
        "    df_majority = df[df[\"has_signal_peptide\"] == 0]\n",
        "    df_minority = df[df[\"has_signal_peptide\"] == 1]\n",
        "\n",
        "    if not df_minority.empty and not df_majority.empty:\n",
        "\n",
        "        n_samples = min(len(df_majority), 5000) # Limit samples to 5000 to prevent high ram usage\n",
        "        df_majority_sampled = resample(\n",
        "            df_majority,\n",
        "            replace=False, # sample without replacement\n",
        "            n_samples=n_samples,\n",
        "            random_state=42\n",
        "        )\n",
        "        df_balanced = pd.concat([df_majority_sampled, df_minority]) # Include all minority samples\n",
        "    else:\n",
        "        df_balanced = df.copy()\n",
        "\n",
        "\n",
        "    # Convert residue-level labels to binary\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "\n",
        "    # Create sliding windows for all sequences\n",
        "    all_windows = []\n",
        "    all_labels = []\n",
        "    all_seq_ids = []\n",
        "\n",
        "    for idx, row in df_balanced.iterrows():\n",
        "        sequence = row[\"sequence\"]\n",
        "        label_string = row[\"label\"]\n",
        "\n",
        "        # Convert label string to binary array\n",
        "        residue_labels = [label_map.get(c, 0) for c in label_string]\n",
        "\n",
        "        # Skip sequences where label length doesn't match sequence length\n",
        "        if len(residue_labels) != len(sequence):\n",
        "            print(\"A sequence length is not equal to the label length\")\n",
        "            continue\n",
        "\n",
        "        # Create sliding windows for this sequence\n",
        "        windows, window_labels, positions = create_sliding_windows(\n",
        "            sequence, residue_labels, WINDOW_SIZE, STRIDE\n",
        "        )\n",
        "\n",
        "        all_windows.extend(windows)\n",
        "        all_labels.extend(window_labels)\n",
        "        all_seq_ids.extend([idx] * len(windows))\n",
        "\n",
        "    print(f\"Total windows created: {len(all_windows)}\")\n",
        "    print(f\"Signal peptide windows: {sum(all_labels)}\")\n",
        "    print(f\"Non-signal peptide windows: {len(all_labels) - sum(all_labels)}\")\n",
        "\n",
        "    return all_windows, all_labels, all_seq_ids, df_balanced\n",
        "\n",
        "def get_protbert_window_embeddings(windows, batch_size=16, output_path=None, embedding_dim=1024):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    If output_path is provided, saves embeddings to a memory-mapped file.\n",
        "    Otherwise, returns a concatenated NumPy array.\n",
        "    \"\"\"\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "    num_windows = len(formatted)\n",
        "    window_size = len(windows[0]) # Assuming all windows have the same size after padding/truncation\n",
        "\n",
        "    if output_path:\n",
        "        # Initialize memory-mapped array\n",
        "        # Need to estimate the exact sequence length after tokenization and potential padding/truncation\n",
        "        # A safer approach is to determine the max length after tokenization or use the known MAX_LENGTH\n",
        "        # Let's use MAX_LENGTH here, assuming it's the effective sequence length after tokenization and padding\n",
        "        print(f\"Initializing memory-mapped file at {output_path} with shape ({num_windows}, {MAX_LENGTH}, {embedding_dim})\")\n",
        "        all_embeddings_mmap = np.memmap(output_path, dtype='float32', mode='w+', shape=(num_windows, MAX_LENGTH, embedding_dim))\n",
        "\n",
        "    all_embeddings_list = [] # Keep this for the case where output_path is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, num_windows, batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            batch_embeddings = outputs.last_hidden_state.cpu().numpy() # (batch, seq_len, emb_dim)\n",
        "\n",
        "            # prot_bert adds [CLS] and [SEP]. We need to slice to get the actual window embeddings\n",
        "            # The actual sequence length before padding can vary within a batch due to truncation\n",
        "            # and the original un-padded window length.\n",
        "            # However, for consistent window embeddings of size MAX_LENGTH, we can just slice\n",
        "            # from index 1 up to MAX_LENGTH + 1 (to exclude CLS and include MAX_LENGTH tokens).\n",
        "            # If padding is present, the embeddings for padding tokens will be there but won't affect\n",
        "            # the actual sequence residues.\n",
        "            processed_batch_embeddings = batch_embeddings[:, 1:MAX_LENGTH+1, :] # Remove [CLS] token embedding\n",
        "\n",
        "            if output_path:\n",
        "                # Write directly to the memory-mapped array\n",
        "                end_idx = min(i + batch_size, num_windows)\n",
        "                all_embeddings_mmap[i:end_idx] = processed_batch_embeddings[:end_idx-i] # Handle the last batch size\n",
        "\n",
        "            else:\n",
        "                # Append to the list if not saving to file\n",
        "                for emb in processed_batch_embeddings:\n",
        "                    all_embeddings_list.append(emb)\n",
        "\n",
        "    if output_path:\n",
        "        # Ensure all changes are written to disk\n",
        "        all_embeddings_mmap.flush()\n",
        "        # The memory-mapped file will be returned. It behaves like a numpy array.\n",
        "        return all_embeddings_mmap\n",
        "    else:\n",
        "        # Return concatenated array\n",
        "        return np.stack(all_embeddings_list)\n",
        "\n",
        "class LazySlidingWindowDataset(Dataset):\n",
        "    def __init__(self, embeddings_path, labels_path, indices):\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.labels_path = labels_path\n",
        "        self.indices = indices # Indices corresponding to the split (train, val, or test)\n",
        "\n",
        "        # Load the full embeddings and labels once\n",
        "        self.all_embeddings = np.load(self.embeddings_path, mmap_mode='r') # Use mmap_mode to avoid loading everything into memory\n",
        "        self.all_labels = np.load(self.labels_path, mmap_mode='r')\n",
        "\n",
        "        # Ensure indices are within bounds (should be handled by splitting logic, but good practice)\n",
        "        if max(indices) >= len(self.all_labels) or min(indices) < 0:\n",
        "             raise ValueError(\"Indices are out of bounds for the loaded data.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the index in the original full dataset\n",
        "        original_idx = self.indices[idx]\n",
        "\n",
        "        # Load the specific embedding and label using the original index\n",
        "        # Slicing with numpy arrays loaded via mmap_mode='r' is efficient\n",
        "        embedding = self.all_embeddings[original_idx]\n",
        "        label = self.all_labels[original_idx]\n",
        "\n",
        "        return {\n",
        "            'window': torch.tensor(embedding, dtype=torch.float32),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifier(nn.Module):\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=2,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = num_aa\n",
        "\n",
        "        for out_channels in cnn_channels:\n",
        "            self.conv_layers.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # LSTM layers for sequential dependencies\n",
        "        # Input to LSTM: [batch_size, seq_len, features]\n",
        "        lstm_input_size = cnn_channels[-1]  # Last CNN output channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate LSTM output size\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Attention mechanism to focus on important positions\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lstm_output_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        classifier_layers = []\n",
        "        in_dim = lstm_output_size\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "        # Binary classification output\n",
        "        classifier_layers.append(nn.Linear(hidden_dim, 1))\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size, seq_len, num_features = x.size()\n",
        "\n",
        "        # need [batch_size, num_aa, window_size] for Conv1d\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "\n",
        "        # Apply CNN layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        # need [batch_size, seq_len, features] for LSTM\n",
        "        x = x.transpose(1, 2)  # [batch_size, window_size, cnn_channels[-1]]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # lstm_out: [batch_size, seq_len, lstm_hidden * directions]\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Weighted sum of LSTM outputs\n",
        "        attended_output = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        # attended_output: [batch_size, lstm_hidden * directions]\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(attended_output)\n",
        "        return logits.squeeze(-1)  # Remove last dimension\n",
        "\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifierV2(nn.Module):\n",
        "    \"\"\"Alternative version with different CNN-LSTM integration\"\"\"\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=1,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv1d(num_aa, cnn_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(cnn_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv1d(cnn_channels[1], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for sequential modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate dimensions\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Global pooling options\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "        cnn_features = self.cnn_backbone(x)\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        x = cnn_features.transpose(1, 2)  # [batch_size, window_size, features]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling over sequence dimension\n",
        "        lstm_out = lstm_out.transpose(1, 2)  # [batch_size, features, seq_len]\n",
        "        pooled = self.global_pool(lstm_out).squeeze(-1)  # [batch_size, features]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs, device,\n",
        "                        lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    \"\"\"Enhanced training function with gradient clipping and better scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # More sophisticated learning rate scheduling\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=patience, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits = model(windows)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if train_batches == 0:\n",
        "            print(\"No successful training batches!\")\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                windows = batch['window'].to(device)\n",
        "                labels = batch['label'].to(device).float()\n",
        "\n",
        "                try:\n",
        "                    logits = model(windows)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    val_correct += (predictions == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_batches == 0:\n",
        "            print(\"No successful validation batches!\")\n",
        "            break\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping and best model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience * 2:  # More patience for complex model\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# compute percentage of false predicted labels\n",
        "def sequence_level_accuracy(labels, predictions):\n",
        "    \"\"\"Compute the accuracy of individual window predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = len(labels) # Total number of windows\n",
        "\n",
        "    # Ensure labels and predictions have the same length\n",
        "    if len(labels) != len(predictions):\n",
        "        print(\"Warning: Length of labels and predictions do not match.\")\n",
        "        # Adjust total to the minimum length if lengths differ\n",
        "        total = min(len(labels), len(predictions))\n",
        "        labels = labels[:total]\n",
        "        predictions = predictions[:total]\n",
        "\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Now comparing individual predictions and labels\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    return correct / total\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the sliding window model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(windows)\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['No Signal', 'Signal']))\n",
        "\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    seq_acc = sequence_level_accuracy(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sequence-level Accuracy: {seq_acc:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def predict_sequence(model, sequence, window_size, device, threshold=0.5):\n",
        "    \"\"\"Predict signal peptide positions for a full sequence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy labels (we don't need them for prediction)\n",
        "    dummy_labels = [0] * len(sequence)\n",
        "\n",
        "    # Create sliding windows\n",
        "    windows, _, positions = create_sliding_windows(sequence, dummy_labels, window_size, stride=1)\n",
        "\n",
        "    # Encode windows\n",
        "    encoded_windows = get_protbert_window_embeddings(windows)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_window in encoded_windows:\n",
        "            window_tensor = torch.tensor(encoded_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            logit = model(window_tensor)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            pred = int(prob > threshold)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return predictions, probabilities, positions\n",
        "\n",
        "# Load and preprocess data with sliding windows\n",
        "# This will create windows and labels but NOT encode them yet\n",
        "windows, labels, seq_ids, df_balanced = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "# --- Step 1: Pre-encode and Save Embeddings ---\n",
        "print(\"Encoding all windows...\")\n",
        "# Process windows in batches and save directly to disk\n",
        "embeddings_path = os.path.join(DRIVE_PATH, \"all_window_embeddings.npy\")\n",
        "labels_path = os.path.join(DRIVE_PATH, \"all_window_labels.npy\")\n",
        "df_balanced_path = os.path.join(DRIVE_PATH, \"df_balanced.csv\") # Save the balanced dataframe for later use if needed\n",
        "\n",
        "# Assuming the first window's embedding size will be consistent\n",
        "dummy_encoding = get_protbert_window_embeddings([windows[0]])\n",
        "embedding_dim = dummy_encoding.shape[-1]\n",
        "del dummy_encoding # Free up memory\n",
        "\n",
        "# Use the modified function to save embeddings incrementally\n",
        "all_embeddings = get_protbert_window_embeddings(\n",
        "    windows,\n",
        "    batch_size=BATCH_SIZE, # Use same batch size as for training/inference\n",
        "    output_path=embeddings_path,\n",
        "    embedding_dim=embedding_dim\n",
        ")\n",
        "\n",
        "# Save labels and balanced dataframe\n",
        "np.save(labels_path, np.array(labels))\n",
        "df_balanced.to_csv(df_balanced_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "print(f\"Labels saved to {labels_path}\")\n",
        "print(f\"Balanced DataFrame saved to {df_balanced_path}\")\n",
        "\n",
        "# --- Step 2 & 3: Create Dataset instances using LazySlidingWindowDataset and Update Training/Evaluation ---\n",
        "\n",
        "# Split indices based on unique sequence IDs to avoid data leakage\n",
        "unique_seq_ids = list(df_balanced.index.unique()) # Use index from df_balanced\n",
        "train_seq_ids, temp_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "val_seq_ids, test_seq_ids = train_test_split(temp_seq_ids, test_size=0.5, random_state=42) # 0.5 of 0.2 = 0.1 test size\n",
        "\n",
        "# Get indices corresponding to each split based on the original df_balanced index\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "print(f\"\\nTrain windows (indices): {len(train_indices)}\")\n",
        "print(f\"Validation windows (indices): {len(val_indices)}\")\n",
        "print(f\"Test windows (indices): {len(test_indices)}\")\n",
        "\n",
        "# Create datasets and loaders using the saved files and indices\n",
        "train_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, train_indices)\n",
        "val_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, val_indices)\n",
        "test_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "# Use the embedding dimension determined from the dummy encoding\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, embedding_dim, hidden_dim=128, num_layers=2\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)"
      ],
      "id": "2HkL4M0u-65x",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records loaded: 25693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2436676345.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"has_signal_peptide\"] = df[\"type\"].map({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total windows created: 806011\n",
            "Signal peptide windows: 153863\n",
            "Non-signal peptide windows: 652148\n",
            "Encoding all windows...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing memory-mapped file at /content/drive/MyDrive/PBLRost/all_window_embeddings.npy with shape (806011, 70, 1024)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/25188 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (32,36,1024) into shape (32,70,1024)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2436676345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;31m# Use the modified function to save embeddings incrementally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m all_embeddings = get_protbert_window_embeddings(\n\u001b[0m\u001b[1;32m    647\u001b[0m     \u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Use same batch size as for training/inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-2436676345.py\u001b[0m in \u001b[0;36mget_protbert_window_embeddings\u001b[0;34m(windows, batch_size, output_path, embedding_dim)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;31m# Write directly to the memory-mapped array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_windows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mall_embeddings_mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_batch_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Handle the last batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (32,36,1024) into shape (32,70,1024)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f70f4bf"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ValueError: could not broadcast input array from shape (32,36,1024) into shape (32,70,1024)` indicates that the shape of the processed batch embeddings (`processed_batch_embeddings`) is (batch_size, 36, 1024), but the memory-mapped array `all_embeddings_mmap` was initialized with a shape that implies a sequence length of 70 (`(num_windows, MAX_LENGTH, embedding_dim)` where `MAX_LENGTH` is 70). The discrepancy (36 vs 70) comes from how the sequence length is handled after tokenization and slicing within the `get_protbert_window_embeddings` function. The slice `batch_embeddings[:, 1:MAX_LENGTH+1, :]` should align with the expected window size after tokenization, which seems to be 35 (WINDOW_SIZE) plus the [SEP] token, resulting in 36. The memory-mapped file shape should be based on this actual resulting dimension, not `MAX_LENGTH`.\n",
        "\n",
        "To fix this, the shape of the memory-mapped array `all_embeddings_mmap` should be initialized using the actual sequence length of the processed embeddings (36 in this case, which corresponds to `WINDOW_SIZE + 1` for the [SEP] token after removing [CLS]), instead of `MAX_LENGTH`. I will remove the dummy encoding call to get the embedding dimension, as the transformer model always outputs embeddings of a fixed size (1024 for ProtBert). The sequence length dimension for the memory-mapped array should be determined by the actual size of `processed_batch_embeddings` after slicing `[:, 1:MAX_LENGTH+1, :]`. However, this slicing is intended to get the embeddings for the `MAX_LENGTH` window. The issue is that the tokenizer output shape is determined by `MAX_LENGTH`, and slicing `[:, 1:MAX_LENGTH+1, :]` should result in a sequence length of `MAX_LENGTH`.\n",
        "\n",
        "Let's re-examine the slicing logic. The tokenizer is called with `max_length=MAX_LENGTH` and `padding=True`. This means the tokenized sequence will have `MAX_LENGTH` tokens + [CLS] + [SEP], so `MAX_LENGTH + 2` total tokens, or `MAX_LENGTH + 1` if the original sequence fits exactly and `truncation=True` removes the last token, or just `MAX_LENGTH` if padding is needed. Given `MAX_LENGTH = 70` and `WINDOW_SIZE = 35`, the window length is 35. The tokenization adds [CLS] and [SEP]. A window of 35 AAs becomes `[CLS] A A ... A [SEP]` which is 37 tokens. Padding will extend this to `MAX_LENGTH + 2` tokens if the window is shorter than `MAX_LENGTH`. The slicing `[:, 1:MAX_LENGTH+1, :]` aims to remove the [CLS] token (index 0) and keep up to `MAX_LENGTH` tokens. This would result in a shape with sequence length `MAX_LENGTH`.\n",
        "\n",
        "The error suggests `processed_batch_embeddings` has a sequence length of 36. This implies the slicing `[:, 1:MAX_LENGTH+1, :]` is effectively doing `[:, 1:37, :]` when `MAX_LENGTH` is 70. This is unexpected. Let's assume the issue is in how the target sequence length is determined for the memory-mapped file. The target sequence length for a window embedding should be `MAX_LENGTH` as defined by the tokenizer/padding, or the actual length of the window + 2 if padding/truncation are not active. Given we use padding and truncation with `max_length=MAX_LENGTH`, the output sequence length from the encoder should conceptually be `MAX_LENGTH` (excluding special tokens if sliced correctly).\n",
        "\n",
        "The slicing `outputs.last_hidden_state[j, 1:seq_len+1, :]` in the original `get_protbert_window_embeddings` (before modification) intended to get the actual sequence length `seq_len` from the *original* window before tokenization/padding/truncation and then slice from index 1 (skipping CLS) up to `seq_len+1`. This would result in variable sequence lengths depending on the original window size.\n",
        "The modified function uses `processed_batch_embeddings = batch_embeddings[:, 1:MAX_LENGTH+1, :]`. This slice intends to get embeddings for a sequence length of `MAX_LENGTH`. The `ValueError` shows it got 36.\n",
        "\n",
        "Let's clarify the expected shape of the processed embeddings. A window of size `WINDOW_SIZE` (35) is tokenized with `max_length=MAX_LENGTH` (70) and `padding=True`, `truncation=True`. ProtBert adds [CLS] and [SEP]. The resulting token sequence will be `[CLS] A A ... A [SEP]` potentially padded or truncated to fit within `MAX_LENGTH`. If a 35 AA window is tokenized, it becomes `[CLS] AA1 AA2 ... AA35 [SEP]`. This is 37 tokens. With `max_length=70` and padding, this sequence would be padded up to 72 tokens (including [CLS] and [SEP]). `outputs.last_hidden_state` would have shape `(batch_size, 72, 1024)`. The slice `[:, 1:MAX_LENGTH+1, :]` i.e., `[:, 1:71, :]` would then result in shape `(batch_size, 70, 1024)`. This aligns with the intended `MAX_LENGTH`.\n",
        "\n",
        "The error shape (32, 36, 1024) is puzzling. It suggests the sequence length is 36 after slicing. This would happen if the slice was `[:, 1:37, :]`. Why would `MAX_LENGTH+1` result in 37 when `MAX_LENGTH` is 70?\n",
        "\n",
        "Let's rethink the slicing logic in `get_protbert_window_embeddings`. The goal is to get the embeddings corresponding to the `WINDOW_SIZE` amino acids of the input window. ProtBert tokenizes \"A A B B ...\" adding [CLS] and [SEP]. So a window of size `W` becomes `[CLS]` W tokens `[SEP]`. The embeddings from the encoder will have shape `(batch_size, W+2, emb_dim)`. If we pad/truncate to `MAX_LENGTH`, the shape becomes `(batch_size, MAX_LENGTH+2, emb_dim)`. We want the embeddings for the original `WINDOW_SIZE` residues. These correspond to indices 1 to `WINDOW_SIZE` in the tokenized sequence. So the slice should be `[:, 1:WINDOW_SIZE+1, :]`. This would give embeddings of shape `(batch_size, WINDOW_SIZE, emb_dim)`. The memory-mapped file should then have shape `(num_windows, WINDOW_SIZE, embedding_dim)`.\n",
        "\n",
        "Let's adjust the `get_protbert_window_embeddings` to slice based on `WINDOW_SIZE` and initialize the memory-mapped array shape based on `WINDOW_SIZE`.\n",
        "\n"
      ],
      "id": "5f70f4bf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "1ULeFx7f_XAM",
        "outputId": "598135fe-390b-4667-c201-cc8474ae8202"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"Rostlab/prot_bert\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_CLASSES = 2  # Binary classification (0: no signal peptide, 1: signal peptide)\n",
        "MAX_LENGTH = 70 # max sequence has len 70 in unpartitioned dataset\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "WINDOW_SIZE = 35  # sliding window (odd because model predicts center residue)\n",
        "STRIDE = 1  # Step size for sliding window\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, \"models/2state_tran_lin_cnn.pt\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)\n",
        "\n",
        "def create_sliding_windows(sequence, labels, window_size, stride=1):\n",
        "    \"\"\"Create sliding windows from sequence and corresponding labels\"\"\"\n",
        "    windows = []\n",
        "    window_labels = []\n",
        "    positions = []\n",
        "\n",
        "    # Pad sequence for edge cases\n",
        "    pad_size = window_size // 2 # so starts classification after padding, at first real encoding\n",
        "    padded_seq = 'X' * pad_size + sequence + 'X' * pad_size\n",
        "    padded_labels = [0] * pad_size + labels + [0] * pad_size\n",
        "\n",
        "    # Create sliding windows\n",
        "    for i in range(0, len(sequence), stride):\n",
        "        start_idx = i\n",
        "        end_idx = i + window_size\n",
        "\n",
        "        if end_idx <= len(padded_seq):\n",
        "            window_seq = padded_seq[start_idx:end_idx]\n",
        "            # Label for the center position of the window\n",
        "            center_idx = start_idx + pad_size # residue to predict\n",
        "            if center_idx < len(padded_labels):\n",
        "                center_label = padded_labels[center_idx]\n",
        "\n",
        "                windows.append(window_seq)\n",
        "                window_labels.append(center_label)\n",
        "                positions.append(i)  # Original position in sequence\n",
        "\n",
        "    return windows, window_labels, positions\n",
        "\n",
        "def get_protbert_window_embeddings(windows, batch_size=16, output_path=None, embedding_dim=1024):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    If output_path is provided, saves embeddings to a memory-mapped file.\n",
        "    Otherwise, returns a concatenated NumPy array.\n",
        "    Slices to get embeddings for the WINDOW_SIZE amino acids.\n",
        "    \"\"\"\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "    num_windows = len(formatted)\n",
        "\n",
        "    # The actual sequence length after tokenization and slicing will be WINDOW_SIZE\n",
        "    target_seq_length = WINDOW_SIZE\n",
        "\n",
        "    if output_path:\n",
        "        # Initialize memory-mapped array with the correct sequence length\n",
        "        print(f\"Initializing memory-mapped file at {output_path} with shape ({num_windows}, {target_seq_length}, {embedding_dim})\")\n",
        "        all_embeddings_mmap = np.memmap(output_path, dtype='float32', mode='w+', shape=(num_windows, target_seq_length, embedding_dim))\n",
        "\n",
        "    all_embeddings_list = [] # Keep this for the case where output_path is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, num_windows, batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            # Use MAX_LENGTH for tokenizer max_length to handle longer sequences,\n",
        "            # but we will slice to WINDOW_SIZE afterwards.\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            batch_embeddings = outputs.last_hidden_state.cpu().numpy() # (batch, tokenized_seq_len, emb_dim)\n",
        "\n",
        "            # Slice to get embeddings for the WINDOW_SIZE amino acids (excluding [CLS] and [SEP])\n",
        "            # These are typically at indices 1 to WINDOW_SIZE in the tokenized sequence if not padded/truncated.\n",
        "            # If padded/truncated, the first WINDOW_SIZE relevant tokens might be padded.\n",
        "            # We need to be careful here. The model expects a fixed size input WINDOW_SIZE.\n",
        "            # Let's assume the slicing should always result in WINDOW_SIZE.\n",
        "            # If the tokenized sequence is shorter than WINDOW_SIZE + 2 after truncation,\n",
        "            # this slice might be out of bounds or return fewer than WINDOW_SIZE embeddings.\n",
        "            # Given MAX_LENGTH is 70 and WINDOW_SIZE is 35, a 35 AA window tokenizes to 37 tokens ([CLS], 35 AA, [SEP]).\n",
        "            # With padding/truncation to MAX_LENGTH=70, the output tensor will be shape (batch, 72, 1024).\n",
        "            # We want the 35 AA embeddings, which are at indices 1 to 36.\n",
        "            # So the slice should be [:, 1:WINDOW_SIZE+1, :]\n",
        "            processed_batch_embeddings = batch_embeddings[:, 1:WINDOW_SIZE+1, :] # Shape (batch, WINDOW_SIZE, emb_dim)\n",
        "\n",
        "\n",
        "            if output_path:\n",
        "                # Write directly to the memory-mapped array\n",
        "                end_idx = min(i + batch_size, num_windows)\n",
        "                # Ensure the batch size written matches the actual batch size\n",
        "                batch_size_actual = processed_batch_embeddings.shape[0]\n",
        "                all_embeddings_mmap[i:i+batch_size_actual] = processed_batch_embeddings # Handle the last batch size\n",
        "\n",
        "            else:\n",
        "                # Append to the list if not saving to file\n",
        "                for emb in processed_batch_embeddings:\n",
        "                    all_embeddings_list.append(emb)\n",
        "\n",
        "    if output_path:\n",
        "        # Ensure all changes are written to disk\n",
        "        all_embeddings_mmap.flush()\n",
        "        # The memory-mapped file will be returned. It behaves like a numpy array.\n",
        "        return all_embeddings_mmap\n",
        "    else:\n",
        "        # Return concatenated array\n",
        "        return np.stack(all_embeddings_list)\n",
        "\n",
        "class LazySlidingWindowDataset(Dataset):\n",
        "    def __init__(self, embeddings_path, labels_path, indices):\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.labels_path = labels_path\n",
        "        self.indices = indices # Indices corresponding to the split (train, val, or test)\n",
        "\n",
        "        # Load the full embeddings and labels once\n",
        "        self.all_embeddings = np.load(self.embeddings_path, mmap_mode='r') # Use mmap_mode to avoid loading everything into memory\n",
        "        self.all_labels = np.load(self.labels_path, mmap_mode='r')\n",
        "\n",
        "        # Ensure indices are within bounds (should be handled by splitting logic, but good practice)\n",
        "        if max(indices) >= len(self.all_labels) or min(indices) < 0:\n",
        "             raise ValueError(\"Indices are out of bounds for the loaded data.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the index in the original full dataset\n",
        "        original_idx = self.indices[idx]\n",
        "\n",
        "        # Load the specific embedding and label using the original index\n",
        "        # Slicing with numpy arrays loaded via mmap_mode='r' is efficient\n",
        "        embedding = self.all_embeddings[original_idx]\n",
        "        label = self.all_labels[original_idx]\n",
        "\n",
        "        return {\n",
        "            'window': torch.tensor(embedding, dtype=torch.float32),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifier(nn.Module):\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=2,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = num_aa\n",
        "\n",
        "        for out_channels in cnn_channels:\n",
        "            self.conv_layers.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # LSTM layers for sequential dependencies\n",
        "        # Input to LSTM: [batch_size, seq_len, features]\n",
        "        lstm_input_size = cnn_channels[-1]  # Last CNN output channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate LSTM output size\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Attention mechanism to focus on important positions\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lstm_output_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        classifier_layers = []\n",
        "        in_dim = lstm_output_size\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "        # Binary classification output\n",
        "        classifier_layers.append(nn.Linear(hidden_dim, 1))\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size, seq_len, num_features = x.size()\n",
        "\n",
        "        # need [batch_size, num_aa, window_size] for Conv1d\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "\n",
        "        # Apply CNN layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        # need [batch_size, seq_len, features] for LSTM\n",
        "        x = x.transpose(1, 2)  # [batch_size, window_size, cnn_channels[-1]]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # lstm_out: [batch_size, seq_len, lstm_hidden * directions]\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Weighted sum of LSTM outputs\n",
        "        attended_output = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        # attended_output: [batch_size, lstm_hidden * directions]\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(attended_output)\n",
        "        return logits.squeeze(-1)  # Remove last dimension\n",
        "\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifierV2(nn.Module):\n",
        "    \"\"\"Alternative version with different CNN-LSTM integration\"\"\"\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=1,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv1d(num_aa, cnn_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(cnn_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv1d(cnn_channels[1], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for sequential modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate dimensions\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Global pooling options\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "        cnn_features = self.cnn_backbone(x)\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        x = cnn_features.transpose(1, 2)  # [batch_size, window_size, features]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling over sequence dimension\n",
        "        lstm_out = lstm_out.transpose(1, 2)  # [batch_size, features, seq_len]\n",
        "        pooled = self.global_pool(lstm_out).squeeze(-1)  # [batch_size, features]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs, device,\n",
        "                        lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    \"\"\"Enhanced training function with gradient clipping and better scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # More sophisticated learning rate scheduling\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=patience, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits = model(windows)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if train_batches == 0:\n",
        "            print(\"No successful training batches!\")\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                windows = batch['window'].to(device)\n",
        "                labels = batch['label'].to(device).float()\n",
        "\n",
        "                try:\n",
        "                    logits = model(windows)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    val_correct += (predictions == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_batches == 0:\n",
        "            print(\"No successful validation batches!\")\n",
        "            break\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping and best model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience * 2:  # More patience for complex model\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# compute percentage of false predicted labels\n",
        "def sequence_level_accuracy(labels, predictions):\n",
        "    \"\"\"Compute the accuracy of individual window predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = len(labels) # Total number of windows\n",
        "\n",
        "    # Ensure labels and predictions have the same length\n",
        "    if len(labels) != len(predictions):\n",
        "        print(\"Warning: Length of labels and predictions do not match.\")\n",
        "        # Adjust total to the minimum length if lengths differ\n",
        "        total = min(len(labels), len(predictions))\n",
        "        labels = labels[:total]\n",
        "        predictions = predictions[:total]\n",
        "\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Now comparing individual predictions and labels\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    return correct / total\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the sliding window model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(windows)\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['No Signal', 'Signal']))\n",
        "\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    seq_acc = sequence_level_accuracy(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sequence-level Accuracy: {seq_acc:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def predict_sequence(model, sequence, window_size, device, threshold=0.5):\n",
        "    \"\"\"Predict signal peptide positions for a full sequence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy labels (we don't need them for prediction)\n",
        "    dummy_labels = [0] * len(sequence)\n",
        "\n",
        "    # Create sliding windows\n",
        "    windows, _, positions = create_sliding_windows(sequence, dummy_labels, window_size, stride=1)\n",
        "\n",
        "    # Encode windows\n",
        "    encoded_windows = get_protbert_window_embeddings(windows)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_window in encoded_windows:\n",
        "            window_tensor = torch.tensor(encoded_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            logit = model(window_tensor)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            pred = int(prob > threshold)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return predictions, probabilities, positions\n",
        "\n",
        "# Load and preprocess data with sliding windows\n",
        "# This will create windows and labels but NOT encode them yet\n",
        "# Assuming load_and_preprocess_data is defined elsewhere and returns windows, labels, seq_ids, df_balanced\n",
        "# Example dummy function (replace with your actual data loading logic):\n",
        "def load_and_preprocess_data(fasta_path):\n",
        "    # This is a placeholder. Replace with your actual data loading.\n",
        "    # It should return a list of window sequences, a list of corresponding labels,\n",
        "    # a list of original sequence IDs for each window, and a DataFrame.\n",
        "    # Example dummy data:\n",
        "    sequences = [\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"ABCDEFGHIJKLMN\"] * 50 # Longer sequence\n",
        "    labels = [[1] * 15 + [0] * (len(seq) - 15) for seq in sequences] # Dummy labels\n",
        "\n",
        "    all_windows = []\n",
        "    all_window_labels = []\n",
        "    all_seq_ids = []\n",
        "    df_rows = []\n",
        "    window_counter = 0\n",
        "\n",
        "    for i, seq in enumerate(sequences):\n",
        "        seq_id = f\"seq_{i}\"\n",
        "        windows, window_labels, positions = create_sliding_windows(seq, labels[i], WINDOW_SIZE, STRIDE)\n",
        "        all_windows.extend(windows)\n",
        "        all_window_labels.extend(window_labels)\n",
        "        all_seq_ids.extend([seq_id] * len(windows))\n",
        "        # Create DataFrame rows\n",
        "        for j in range(len(windows)):\n",
        "             df_rows.append({'sequence_id': seq_id, 'window_index': j, 'window_sequence': windows[j], 'label': window_labels[j], 'position_in_sequence': positions[j]}) # Add position if needed\n",
        "             window_counter += 1\n",
        "\n",
        "    df_balanced = pd.DataFrame(df_rows)\n",
        "    df_balanced = df_balanced.set_index('sequence_id') # Set index to seq_id for splitting\n",
        "    # Ensure the index name is set explicitly\n",
        "    df_balanced.index.name = 'sequence_id'\n",
        "\n",
        "    print(f\"Created {len(all_windows)} sliding windows.\")\n",
        "    return all_windows, all_window_labels, all_seq_ids, df_balanced\n",
        "\n",
        "\n",
        "windows, labels, seq_ids, df_balanced = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "# --- Step 1: Pre-encode and Save Embeddings ---\n",
        "print(\"Encoding all windows...\")\n",
        "# Process windows in batches and save directly to disk\n",
        "embeddings_path = os.path.join(DRIVE_PATH, \"all_window_embeddings.npy\")\n",
        "labels_path = os.path.join(DRIVE_PATH, \"all_window_labels.npy\")\n",
        "df_balanced_path = os.path.join(DRIVE_PATH, \"df_balanced.csv\") # Save the balanced dataframe for later use if needed\n",
        "\n",
        "# The embedding dimension is fixed by ProtBert-BFD\n",
        "embedding_dim = 1024 # ProtBert-BFD embedding dimension\n",
        "\n",
        "# Use the modified function to save embeddings incrementally\n",
        "all_embeddings = get_protbert_window_embeddings(\n",
        "    windows,\n",
        "    batch_size=BATCH_SIZE, # Use same batch size as for training/inference\n",
        "    output_path=embeddings_path,\n",
        "    embedding_dim=embedding_dim # Pass the known embedding dimension\n",
        ")\n",
        "\n",
        "# Save labels and balanced dataframe\n",
        "np.save(labels_path, np.array(labels))\n",
        "df_balanced.to_csv(df_balanced_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "print(f\"Labels saved to {labels_path}\")\n",
        "print(f\"Balanced DataFrame saved to {df_balanced_path}\")\n",
        "\n",
        "# --- Step 2 & 3: Create Dataset instances using LazySlidingWindowDataset and Update Training/Evaluation ---\n",
        "\n",
        "# Split indices based on unique sequence IDs to avoid data leakage\n",
        "unique_seq_ids = list(df_balanced.index.unique()) # Use index from df_balanced\n",
        "train_seq_ids, temp_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "val_seq_ids, test_seq_ids = train_test_split(temp_seq_ids, test_size=0.5, random_state=42) # 0.5 of 0.2 = 0.1 test size\n",
        "\n",
        "# Get indices corresponding to each split based on the original df_balanced index\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "print(f\"\\nTrain windows (indices): {len(train_indices)}\")\n",
        "print(f\"Validation windows (indices): {len(val_indices)}\")\n",
        "print(f\"Test windows (indices): {len(test_indices)}\")\n",
        "\n",
        "# Create datasets and loaders using the saved files and indices\n",
        "train_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, train_indices)\n",
        "val_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, val_indices)\n",
        "test_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "# Use the embedding dimension determined from the dummy encoding\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, embedding_dim, hidden_dim=128, num_layers=2\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)\n"
      ],
      "id": "1ULeFx7f_XAM",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Created 2000 sliding windows.\n",
            "Encoding all windows...\n",
            "Initializing memory-mapped file at /content/drive/MyDrive/PBLRost/all_window_embeddings.npy with shape (2000, 35, 1024)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:20<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to /content/drive/MyDrive/PBLRost/all_window_embeddings.npy\n",
            "Labels saved to /content/drive/MyDrive/PBLRost/all_window_labels.npy\n",
            "Balanced DataFrame saved to /content/drive/MyDrive/PBLRost/df_balanced.csv\n",
            "\n",
            "Train windows (indices): 1576\n",
            "Validation windows (indices): 224\n",
            "Test windows (indices): 200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot load file containing pickled data when allow_pickle=False",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-277698470.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;31m# Create datasets and loaders using the saved files and indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazySlidingWindowDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazySlidingWindowDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLazySlidingWindowDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-277698470.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings_path, labels_path, indices)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Load the full embeddings and labels once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use mmap_mode to avoid loading everything into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    491\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e58c926e"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ValueError: Cannot load file containing pickled data when allow_pickle=False` indicates that the `.npy` file being loaded contains pickled Python objects, and `np.load` is being called with `allow_pickle=False`, which is the default and recommended for security. This happens because the dummy `load_and_preprocess_data` function I added for testing created a small, artificial DataFrame and lists, and `np.save` on these might result in pickling. The actual `load_and_preprocess_data` function (which was defined in a previous cell and seems to have been overwritten by the dummy one in the last execution) processes real FASTA data and should produce standard NumPy arrays/lists that don't require pickling when saved.\n",
        "\n",
        "The fix is to revert to the correct `load_and_preprocess_data` function definition and remove the dummy version. I will also ensure the `np.load` calls in `LazySlidingWindowDataset` explicitly set `allow_pickle=False` to maintain security and verify that the data being saved is not pickled. The error suggests the labels file might also be the issue, so I will ensure both `np.load` calls in the dataset class use `allow_pickle=False`.\n",
        "\n"
      ],
      "id": "e58c926e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBkXBfEe_rwJ",
        "outputId": "60a8e6dc-3726-45fa-e6b5-6f210489de2c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"Rostlab/prot_bert\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_CLASSES = 2  # Binary classification (0: no signal peptide, 1: signal peptide)\n",
        "MAX_LENGTH = 70 # max sequence has len 70 in unpartitioned dataset\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "WINDOW_SIZE = 35  # sliding window (odd because model predicts center residue)\n",
        "STRIDE = 1  # Step size for sliding window\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, \"models/2state_tran_lin_cnn.pt\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)\n",
        "\n",
        "def create_sliding_windows(sequence, labels, window_size, stride=1):\n",
        "    \"\"\"Create sliding windows from sequence and corresponding labels\"\"\"\n",
        "    windows = []\n",
        "    window_labels = []\n",
        "    positions = []\n",
        "\n",
        "    # Pad sequence for edge cases\n",
        "    pad_size = window_size // 2 # so starts classification after padding, at first real encoding\n",
        "    padded_seq = 'X' * pad_size + sequence + 'X' * pad_size\n",
        "    padded_labels = [0] * pad_size + labels + [0] * pad_size\n",
        "\n",
        "    # Create sliding windows\n",
        "    for i in range(0, len(sequence), stride):\n",
        "        start_idx = i\n",
        "        end_idx = i + window_size\n",
        "\n",
        "        if end_idx <= len(padded_seq):\n",
        "            window_seq = padded_seq[start_idx:end_idx]\n",
        "            # Label for the center position of the window\n",
        "            center_idx = start_idx + pad_size # residue to predict\n",
        "            if center_idx < len(padded_labels):\n",
        "                center_label = padded_labels[center_idx]\n",
        "\n",
        "                windows.append(window_seq)\n",
        "                window_labels.append(center_label)\n",
        "                positions.append(i)  # Original position in sequence\n",
        "\n",
        "    return windows, window_labels, positions\n",
        "\n",
        "def load_and_preprocess_data(fasta_path):\n",
        "    \"\"\"Load FASTA data and preprocess for sliding window approach\"\"\"\n",
        "    records = []\n",
        "\n",
        "    with open(fasta_path, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                        records.append(current_record)\n",
        "\n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\n",
        "                    \"uniprot_ac\": uniprot_ac,\n",
        "                    \"kingdom\": kingdom,\n",
        "                    \"type\": type_,\n",
        "                    \"sequence\": None,\n",
        "                    \"label\": None\n",
        "                }\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "\n",
        "        # Add last record\n",
        "        if current_record is not None:\n",
        "            if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                records.append(current_record)\n",
        "\n",
        "    print(f\"Total records loaded: {len(records)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_raw = pd.DataFrame(records)\n",
        "\n",
        "    # Filter out sequences with 'P' in labels (if needed)\n",
        "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
        "\n",
        "    # Map signal peptide types to binary classification\n",
        "    df[\"has_signal_peptide\"] = df[\"type\"].map({\n",
        "        \"NO_SP\": 0,\n",
        "        \"LIPO\": 1,\n",
        "        \"SP\": 1,\n",
        "        \"TAT\": 1,\n",
        "        \"TATLIPO\": 1\n",
        "    })\n",
        "\n",
        "    # Balance the dataset at sequence level first\n",
        "    df_majority = df[df[\"has_signal_peptide\"] == 0]\n",
        "    df_minority = df[df[\"has_signal_peptide\"] == 1]\n",
        "\n",
        "    if not df_minority.empty and not df_majority.empty:\n",
        "\n",
        "        n_samples = min(len(df_majority), 5000) # Limit samples to 5000 to prevent high ram usage\n",
        "        df_majority_sampled = resample(\n",
        "            df_majority,\n",
        "            replace=False, # sample without replacement\n",
        "            n_samples=n_samples,\n",
        "            random_state=42\n",
        "        )\n",
        "        df_balanced = pd.concat([df_majority_sampled, df_minority]) # Include all minority samples\n",
        "    else:\n",
        "        df_balanced = df.copy()\n",
        "\n",
        "\n",
        "    # Convert residue-level labels to binary\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "\n",
        "    # Create sliding windows for all sequences\n",
        "    all_windows = []\n",
        "    all_labels = []\n",
        "    all_seq_ids = []\n",
        "\n",
        "    for idx, row in df_balanced.iterrows():\n",
        "        sequence = row[\"sequence\"]\n",
        "        label_string = row[\"label\"]\n",
        "\n",
        "        # Convert label string to binary array\n",
        "        residue_labels = [label_map.get(c, 0) for c in label_string]\n",
        "\n",
        "        # Skip sequences where label length doesn't match sequence length\n",
        "        if len(residue_labels) != len(sequence):\n",
        "            print(\"A sequence length is not equal to the label length\")\n",
        "            continue\n",
        "\n",
        "        # Create sliding windows for this sequence\n",
        "        windows, window_labels, positions = create_sliding_windows(\n",
        "            sequence, residue_labels, WINDOW_SIZE, STRIDE\n",
        "        )\n",
        "\n",
        "        all_windows.extend(windows)\n",
        "        all_labels.extend(window_labels)\n",
        "        all_seq_ids.extend([idx] * len(windows))\n",
        "\n",
        "    print(f\"Total windows created: {len(all_windows)}\")\n",
        "    print(f\"Signal peptide windows: {sum(all_labels)}\")\n",
        "    print(f\"Non-signal peptide windows: {len(all_labels) - sum(all_labels)}\")\n",
        "\n",
        "    return all_windows, all_labels, all_seq_ids, df_balanced\n",
        "\n",
        "def get_protbert_window_embeddings(windows, batch_size=16, output_path=None, embedding_dim=1024):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    If output_path is provided, saves embeddings to a memory-mapped file.\n",
        "    Otherwise, returns a concatenated NumPy array.\n",
        "    Slices to get embeddings for the WINDOW_SIZE amino acids.\n",
        "    \"\"\"\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "    num_windows = len(formatted)\n",
        "\n",
        "    # The actual sequence length after tokenization and slicing will be WINDOW_SIZE\n",
        "    target_seq_length = WINDOW_SIZE\n",
        "\n",
        "    if output_path:\n",
        "        # Initialize memory-mapped array with the correct sequence length\n",
        "        print(f\"Initializing memory-mapped file at {output_path} with shape ({num_windows}, {target_seq_length}, {embedding_dim})\")\n",
        "        all_embeddings_mmap = np.memmap(output_path, dtype='float32', mode='w+', shape=(num_windows, target_seq_length, embedding_dim))\n",
        "\n",
        "    all_embeddings_list = [] # Keep this for the case where output_path is None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, num_windows, batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            # Use MAX_LENGTH for tokenizer max_length to handle longer sequences,\n",
        "            # but we will slice to WINDOW_SIZE afterwards.\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            batch_embeddings = outputs.last_hidden_state.cpu().numpy() # (batch, tokenized_seq_len, emb_dim)\n",
        "\n",
        "            # Slice to get embeddings for the WINDOW_SIZE amino acids (excluding [CLS] and [SEP])\n",
        "            # These are typically at indices 1 to WINDOW_SIZE in the tokenized sequence if not padded/truncated.\n",
        "            # If padded/truncated, the first WINDOW_SIZE relevant tokens might be padded.\n",
        "            # We need to be careful here. The model expects a fixed size input WINDOW_SIZE.\n",
        "            # Let's assume the slicing should always result in WINDOW_SIZE.\n",
        "            # If the tokenized sequence is shorter than WINDOW_SIZE + 2 after truncation,\n",
        "            # this slice might be out of bounds or return fewer than WINDOW_SIZE embeddings.\n",
        "            # Given MAX_LENGTH is 70 and WINDOW_SIZE is 35, a 35 AA window tokenizes to 37 tokens ([CLS], 35 AA, [SEP]).\n",
        "            # With padding/truncation to MAX_LENGTH=70, the output tensor will be shape (batch, 72, 1024).\n",
        "            # We want the 35 AA embeddings, which are at indices 1 to 36.\n",
        "            # So the slice should be [:, 1:WINDOW_SIZE+1, :]\n",
        "            processed_batch_embeddings = batch_embeddings[:, 1:WINDOW_SIZE+1, :] # Shape (batch, WINDOW_SIZE, emb_dim)\n",
        "\n",
        "\n",
        "            if output_path:\n",
        "                # Write directly to the memory-mapped array\n",
        "                end_idx = min(i + batch_size, num_windows)\n",
        "                # Ensure the batch size written matches the actual batch size\n",
        "                batch_size_actual = processed_batch_embeddings.shape[0]\n",
        "                all_embeddings_mmap[i:i+batch_size_actual] = processed_batch_embeddings # Handle the last batch size\n",
        "\n",
        "            else:\n",
        "                # Append to the list if not saving to file\n",
        "                for emb in processed_batch_embeddings:\n",
        "                    all_embeddings_list.append(emb)\n",
        "\n",
        "    if output_path:\n",
        "        # Ensure all changes are written to disk\n",
        "        all_embeddings_mmap.flush()\n",
        "        # The memory-mapped file will be returned. It behaves like a numpy array.\n",
        "        return all_embeddings_mmap\n",
        "    else:\n",
        "        # Return concatenated array\n",
        "        return np.stack(all_embeddings_list)\n",
        "\n",
        "\n",
        "class LazySlidingWindowDataset(Dataset):\n",
        "    def __init__(self, embeddings_path, labels_path, indices):\n",
        "        self.embeddings_path = embeddings_path\n",
        "        self.labels_path = labels_path\n",
        "        self.indices = indices # Indices corresponding to the split (train, val, or test)\n",
        "\n",
        "        # Load the full embeddings and labels once, ensure allow_pickle=False\n",
        "        self.all_embeddings = np.load(self.embeddings_path, mmap_mode='r', allow_pickle=False) # Use mmap_mode to avoid loading everything into memory\n",
        "        self.all_labels = np.load(self.labels_path, mmap_mode='r', allow_pickle=False)\n",
        "\n",
        "        # Ensure indices are within bounds (should be handled by splitting logic, but good practice)\n",
        "        if max(indices) >= len(self.all_labels) or min(indices) < 0:\n",
        "             raise ValueError(\"Indices are out of bounds for the loaded data.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the index in the original full dataset\n",
        "        original_idx = self.indices[idx]\n",
        "\n",
        "        # Load the specific embedding and label using the original index\n",
        "        # Slicing with numpy arrays loaded via mmap_mode='r' is efficient\n",
        "        embedding = self.all_embeddings[original_idx]\n",
        "        label = self.all_labels[original_idx]\n",
        "\n",
        "        return {\n",
        "            'window': torch.tensor(embedding, dtype=torch.float32),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifier(nn.Module):\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=2,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = num_aa\n",
        "\n",
        "        for out_channels in cnn_channels:\n",
        "            self.conv_layers.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # LSTM layers for sequential dependencies\n",
        "        # Input to LSTM: [batch_size, seq_len, features]\n",
        "        lstm_input_size = cnn_channels[-1]  # Last CNN output channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate LSTM output size\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Attention mechanism to focus on important positions\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lstm_output_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        classifier_layers = []\n",
        "        in_dim = lstm_output_size\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "        # Binary classification output\n",
        "        classifier_layers.append(nn.Linear(hidden_dim, 1))\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size, seq_len, num_features = x.size()\n",
        "\n",
        "        # need [batch_size, num_aa, window_size] for Conv1d\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "\n",
        "        # Apply CNN layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        # need [batch_size, seq_len, features] for LSTM\n",
        "        x = x.transpose(1, 2)  # [batch_size, window_size, cnn_channels[-1]]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # lstm_out: [batch_size, seq_len, lstm_hidden * directions]\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Weighted sum of LSTM outputs\n",
        "        attended_output = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        # attended_output: [batch_size, lstm_hidden * directions]\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(attended_output)\n",
        "        return logits.squeeze(-1)  # Remove last dimension\n",
        "\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifierV2(nn.Module):\n",
        "    \"\"\"Alternative version with different CNN-LSTM integration\"\"\"\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=1,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv1d(num_aa, cnn_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(cnn_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv1d(cnn_channels[1], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for sequential modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate dimensions\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Global pooling options\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "        cnn_features = self.cnn_backbone(x)\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        x = cnn_features.transpose(1, 2)  # [batch_size, window_size, features]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling over sequence dimension\n",
        "        lstm_out = lstm_out.transpose(1, 2)  # [batch_size, features, seq_len]\n",
        "        pooled = self.global_pool(lstm_out).squeeze(-1)  # [batch_size, features]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits.squeeze(-1)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs, device,\n",
        "                        lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    \"\"\"Enhanced training function with gradient clipping and better scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # More sophisticated learning rate scheduling\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=patience, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits = model(windows)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if train_batches == 0:\n",
        "            print(\"No successful training batches!\")\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                windows = batch['window'].to(device)\n",
        "                labels = batch['label'].to(device).float()\n",
        "\n",
        "                try:\n",
        "                    logits = model(windows)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    val_correct += (predictions == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_batches == 0:\n",
        "            print(\"No successful validation batches!\")\n",
        "            break\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping and best model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience * 2:  # More patience for complex model\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# compute percentage of false predicted labels\n",
        "def sequence_level_accuracy(labels, predictions):\n",
        "    \"\"\"Compute the accuracy of individual window predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = len(labels) # Total number of windows\n",
        "\n",
        "    # Ensure labels and predictions have the same length\n",
        "    if len(labels) != len(predictions):\n",
        "        print(\"Warning: Length of labels and predictions do not match.\")\n",
        "        # Adjust total to the minimum length if lengths differ\n",
        "        total = min(len(labels), len(predictions))\n",
        "        labels = labels[:total]\n",
        "        predictions = predictions[:total]\n",
        "\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Now comparing individual predictions and labels\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    return correct / total\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the sliding window model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(windows)\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['No Signal', 'Signal']))\n",
        "\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    seq_acc = sequence_level_accuracy(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sequence-level Accuracy: {seq_acc:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def predict_sequence(model, sequence, window_size, device, threshold=0.5):\n",
        "    \"\"\"Predict signal peptide positions for a full sequence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy labels (we don't need them for prediction)\n",
        "    dummy_labels = [0] * len(sequence)\n",
        "\n",
        "    # Create sliding windows\n",
        "    windows, _, positions = create_sliding_windows(sequence, dummy_labels, window_size, stride=1)\n",
        "\n",
        "    # Encode windows\n",
        "    # Note: This predict_sequence function still uses the get_protbert_window_embeddings\n",
        "    # without the output_path, meaning it will load all embeddings into memory for prediction.\n",
        "    # For very long sequences, this might still cause high RAM usage.\n",
        "    # A better approach for prediction would be to process windows one by one or in small batches\n",
        "    # without saving to a memory-mapped file first.\n",
        "    encoded_windows = get_protbert_window_embeddings(windows)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_window in encoded_windows:\n",
        "            window_tensor = torch.tensor(encoded_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            logit = model(window_tensor)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            pred = int(prob > threshold)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return predictions, probabilities, positions\n",
        "\n",
        "# Load and preprocess data with sliding windows\n",
        "# This will create windows and labels but NOT encode them yet\n",
        "windows, labels, seq_ids, df_balanced = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "# --- Step 1: Pre-encode and Save Embeddings ---\n",
        "print(\"Encoding all windows...\")\n",
        "# Process windows in batches and save directly to disk\n",
        "embeddings_path = os.path.join(DRIVE_PATH, \"all_window_embeddings.npy\")\n",
        "labels_path = os.path.join(DRIVE_PATH, \"all_window_labels.npy\")\n",
        "df_balanced_path = os.path.join(DRIVE_PATH, \"df_balanced.csv\") # Save the balanced dataframe for later use if needed\n",
        "\n",
        "# The embedding dimension is fixed by ProtBert-BFD\n",
        "embedding_dim = 1024 # ProtBert-BFD embedding dimension\n",
        "\n",
        "# Use the modified function to save embeddings incrementally\n",
        "all_embeddings = get_protbert_window_embeddings(\n",
        "    windows,\n",
        "    batch_size=BATCH_SIZE, # Use same batch size as for training/inference\n",
        "    output_path=embeddings_path,\n",
        "    embedding_dim=embedding_dim # Pass the known embedding dimension\n",
        ")\n",
        "\n",
        "# Save labels and balanced dataframe\n",
        "np.save(labels_path, np.array(labels))\n",
        "df_balanced.to_csv(df_balanced_path, index=False)\n",
        "\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "print(f\"Labels saved to {labels_path}\")\n",
        "print(f\"Balanced DataFrame saved to {df_balanced_path}\")\n",
        "\n",
        "# --- Step 2 & 3: Create Dataset instances using LazySlidingWindowDataset and Update Training/Evaluation ---\n",
        "\n",
        "# Split indices based on unique sequence IDs to avoid data leakage\n",
        "unique_seq_ids = list(df_balanced.index.unique()) # Use index from df_balanced\n",
        "train_seq_ids, temp_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "val_seq_ids, test_seq_ids = train_test_split(temp_seq_ids, test_size=0.5, random_state=42) # 0.5 of 0.2 = 0.1 test size\n",
        "\n",
        "# Get indices corresponding to each split based on the original df_balanced index\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "print(f\"\\nTrain windows (indices): {len(train_indices)}\")\n",
        "print(f\"Validation windows (indices): {len(val_indices)}\")\n",
        "print(f\"Test windows (indices): {len(test_indices)}\")\n",
        "\n",
        "# Create datasets and loaders using the saved files and indices\n",
        "train_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, train_indices)\n",
        "val_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, val_indices)\n",
        "test_dataset = LazySlidingWindowDataset(embeddings_path, labels_path, test_indices)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, embedding_dim, hidden_dim=128, num_layers=2\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)"
      ],
      "id": "IBkXBfEe_rwJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total records loaded: 25693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-1354209157.py:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"has_signal_peptide\"] = df[\"type\"].map({\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total windows created: 806011\n",
            "Signal peptide windows: 153863\n",
            "Non-signal peptide windows: 652148\n",
            "Encoding all windows...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dc63772"
      },
      "source": [
        "## Review and refine\n",
        "\n",
        "### Subtask:\n",
        "Review the code to ensure efficient memory usage throughout the process.\n"
      ],
      "id": "1dc63772"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f426156"
      },
      "source": [
        "**Reasoning**:\n",
        "Review all code cells to ensure efficient memory usage, specifically checking data loading, preprocessing, embedding, dataset handling, and prediction functions for unintended large in-memory objects and verifying the correct use of memory-mapping.\n",
        "\n"
      ],
      "id": "5f426156"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b39441fc",
        "outputId": "d1bc2251-52f5-4cb4-aab7-6eaeb5ae6ac1"
      },
      "source": [
        "# Review of memory usage:\n",
        "\n",
        "# 1. load_and_preprocess_data:\n",
        "# - Reads the FASTA file line by line, which is memory-efficient.\n",
        "# - Stores records in a list, then converts to a pandas DataFrame (df_raw). This could consume memory for large datasets, but pandas is generally optimized.\n",
        "# - Filters the DataFrame (df).\n",
        "# - Balances the DataFrame (df_balanced) by concatenating slices. This creates a new DataFrame but doesn't hold multiple full copies simultaneously.\n",
        "# - Creates all_windows, all_labels, and all_seq_ids lists. These lists can become very large as they store all windows and their corresponding labels/ids in memory. This is a potential area for high RAM usage if the number of windows is extremely large. However, this is necessary to get the total number of windows and their sequence IDs before splitting and creating the memory-mapped embeddings file.\n",
        "# - Returns these lists and the balanced DataFrame.\n",
        "\n",
        "# 2. get_protbert_window_embeddings:\n",
        "# - Takes the 'windows' list as input, which can be large (see point 1).\n",
        "# - Creates 'formatted' list by joining window characters. This duplicates the window data in memory.\n",
        "# - Uses torch.no_grad() which is good for memory during inference/embedding.\n",
        "# - Processes batches of windows.\n",
        "# - If output_path is provided:\n",
        "#   - Initializes a memory-mapped array (all_embeddings_mmap) with mode 'w+'. This allocates space on disk and provides a view that can be written to incrementally, avoiding loading the entire embedding matrix into RAM. Shape initialization uses MAX_LENGTH and embedding_dim, which was corrected in the previous step to use WINDOW_SIZE for the sequence dimension of the embedding.\n",
        "#   - Writes batch_embeddings directly to the mmap array. This is memory efficient.\n",
        "#   - Flushes the mmap array.\n",
        "#   - Returns the mmap object, which acts like a numpy array but reads/writes from disk. This is efficient.\n",
        "# - If output_path is NOT provided:\n",
        "#   - Appends embeddings to all_embeddings_list. This list can grow very large, potentially causing high RAM usage if not saving to a file.\n",
        "#   - Stacks the list into a numpy array. This requires enough RAM to hold the final array.\n",
        "# - The use of output_path addresses the high RAM usage during the embedding generation phase by offloading storage to disk.\n",
        "\n",
        "# 3. LazySlidingWindowDataset:\n",
        "# - Takes embeddings_path, labels_path, and indices as input.\n",
        "# - Uses np.load with mmap_mode='r' and allow_pickle=False to load the embedding and label files. mmap_mode='r' means the data is read from disk only when accessed (getitem), not loaded entirely into RAM during initialization. This is highly memory efficient for large datasets.\n",
        "# - __getitem__ accesses a specific slice of the mmap arrays based on the provided index, loading only that small piece of data into memory at a time. This is efficient.\n",
        "\n",
        "# 4. Training/Evaluation Loops (train_model, evaluate_model):\n",
        "# - Use DataLoaders with the LazySlidingWindowDataset. DataLoaders fetch data in batches, and the dataset loads data lazily from the mmap file. This ensures that only a small batch of embeddings and labels is loaded into GPU/CPU memory at any given time, significantly reducing RAM usage during training and evaluation.\n",
        "# - Model parameters are on the specified DEVICE (GPU/CPU), not necessarily in main RAM in large quantities unless the model is huge.\n",
        "# - Gradients and optimizer states are also managed by PyTorch and are typically on the DEVICE during training.\n",
        "# - Metrics calculation (`classification_report`, `f1_score`, etc.) in `evaluate_model` collects all predictions and labels into lists (`all_preds`, `all_labels`, `all_probs`) which could consume significant RAM if the test set is very large. However, this is generally a post-processing step after inference, and the memory usage during inference itself is managed by batching and lazy loading.\n",
        "\n",
        "# 5. Variable Management:\n",
        "# - The main script creates 'windows', 'labels', and 'seq_ids' lists in `load_and_preprocess_data`. These are then used to determine the size for the mmap file and to generate train/val/test indices. After the mmap file is created, these lists are no longer strictly needed in their entirety for the dataset/dataloader creation, but they are kept in the current scope. While Python's GC should handle them when they are no longer referenced, explicitly deleting them (`del windows, labels, seq_ids`) after the mmap file is successfully created and indices are derived *could* potentially free up that memory sooner if it becomes a bottleneck, but this is often unnecessary.\n",
        "# - The `all_embeddings` variable returned by `get_protbert_window_embeddings` when `output_path` is provided is the mmap object, which is memory efficient.\n",
        "\n",
        "# 6. predict_sequence:\n",
        "# - Creates windows and dummy_labels lists, similar to `load_and_preprocess_data`, which can consume memory for long sequences.\n",
        "# - Calls `get_protbert_window_embeddings` without an `output_path`. This means the function will collect *all* embeddings for the sequence's windows into an in-memory NumPy array before returning it. For a very long sequence, this could lead to high RAM usage.\n",
        "# - Iterates through the `encoded_windows` numpy array batch by batch (implicitly, one window at a time here) for prediction. The memory issue is in the creation of `encoded_windows` itself.\n",
        "# - To address high RAM usage for prediction on very long sequences, `predict_sequence` should be modified to process windows in small batches directly, without first creating the full `encoded_windows` numpy array in memory. It should call the model batch-wise.\n",
        "\n",
        "# Summary of memory efficiency:\n",
        "# - Data loading is reasonably efficient.\n",
        "# - Embedding generation is made efficient by using memory-mapping.\n",
        "# - Dataset loading for training/validation/testing is highly efficient due to lazy loading from the mmap file.\n",
        "# - Training and evaluation loops process data in batches, keeping memory usage manageable.\n",
        "# - Prediction for a single sequence is currently NOT memory efficient for very long sequences as it loads all window embeddings into memory.\n",
        "\n",
        "# Potential areas for improvement (optional, but good practice):\n",
        "# - Explicitly delete the 'windows', 'labels', and 'seq_ids' lists after they are used to create the mmap file and indices in the main script if memory is still a concern.\n",
        "# - Modify `predict_sequence` to process windows in batches without creating a full in-memory array of embeddings.\n",
        "\n",
        "print(\"Memory usage review complete.\")\n",
        "print(\"Identified potential high RAM usage in:\")\n",
        "print(\"1. Storing all window sequences and labels in lists ('windows', 'labels', 'seq_ids') in `load_and_preprocess_data` before creating the memory-mapped file. This is a temporary peak.\")\n",
        "print(\"2. The `predict_sequence` function, which loads all window embeddings for a single sequence into memory.\")\n",
        "print(\"Memory efficiency is good for embedding generation, training, and evaluation due to memory-mapping and batching.\")\n",
        "\n",
        "# No code changes required in this step, only review and reporting."
      ],
      "id": "b39441fc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage review complete.\n",
            "Identified potential high RAM usage in:\n",
            "1. Storing all window sequences and labels in lists ('windows', 'labels', 'seq_ids') in `load_and_preprocess_data` before creating the memory-mapped file. This is a temporary peak.\n",
            "2. The `predict_sequence` function, which loads all window embeddings for a single sequence into memory.\n",
            "Memory efficiency is good for embedding generation, training, and evaluation due to memory-mapping and batching.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47f91691"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial training loop output showing only 3 batches was not addressed in the provided solving process steps. The focus of the steps was on optimizing memory usage during embedding generation and data loading for training.\n",
        "*   The `get_protbert_window_embeddings` function was successfully modified to use `np.memmap` with `mode='w+'` to save embeddings directly to disk in batches, avoiding loading the entire embedding matrix into RAM.\n",
        "*   The `LazySlidingWindowDataset` was implemented to load embeddings and labels from memory-mapped files using `mmap_mode='r'`, enabling lazy loading of individual data samples during training and evaluation, thus reducing memory footprint.\n",
        "*   The main execution cell was updated to use the modified embedding generation function with an output path and to create `DataLoader` instances using the `LazySlidingWindowDataset`.\n",
        "*   A review of the code confirmed that memory usage is efficient during embedding generation and during model training/evaluation due to memory-mapping and lazy loading.\n",
        "*   Potential high RAM usage areas were identified:\n",
        "    *   Temporarily storing all window sequences and labels in lists in the `load_and_preprocess_data` function.\n",
        "    *   Loading all window embeddings for a single sequence into memory within the `predict_sequence` function.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate why the training loop initially showed only 3 batches, as this issue was not resolved by the memory optimization steps. This might relate to data size, batch size, or DataLoader configuration.\n",
        "*   Modify the `predict_sequence` function to process window embeddings in small batches directly from the model without creating a full in-memory NumPy array of embeddings, to improve memory efficiency for inference on long sequences.\n"
      ],
      "id": "47f91691"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}