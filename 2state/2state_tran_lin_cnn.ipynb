{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b1192d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1192d4",
        "outputId": "25388842-9907-4a49-ea26-e127d9333f92"
      },
      "outputs": [],
      "source": [
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install torch\n",
        "%pip install scikit-learn\n",
        "%pip install matplotlib\n",
        "%pip install tqdm\n",
        "%pip install google\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, matthews_corrcoef, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"Rostlab/prot_bert\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_CLASSES = 2  # Binary classification (0: no signal peptide, 1: signal peptide)\n",
        "MAX_LENGTH = 70 # max sequence has len 70 in unpartitioned dataset\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 0.001\n",
        "WINDOW_SIZE = 35  # sliding window (odd because model predicts center residue)\n",
        "STRIDE = 1  # Step size for sliding window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181f6793",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181f6793",
        "outputId": "ce12a7d2-87a9-47d6-fc66-9b6ddddbe4bf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "MODEL_PATH = os.path.join(DRIVE_PATH, \"models/2state_tran_lin_cnn_v3.pt\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
        "encoder = BertModel.from_pretrained(MODEL_NAME)\n",
        "encoder.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce93383",
      "metadata": {
        "id": "6ce93383"
      },
      "outputs": [],
      "source": [
        "def get_protbert_window_embeddings(windows, batch_size=16):\n",
        "    \"\"\"\n",
        "    Output shape: (num_windows, window_size, embedding_dim)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    formatted = [\" \".join(list(window)) for window in windows] # needed for tokenization\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(formatted), batch_size)):\n",
        "            batch_seqs = formatted[i:i+batch_size]\n",
        "            encoded = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "            input_ids = encoded['input_ids'].to(DEVICE)\n",
        "            attention_mask = encoded['attention_mask'].to(DEVICE)\n",
        "\n",
        "            outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            # outputs.last_hidden_state: (batch, seq_len, emb_dim)\n",
        "            # Remove [CLS] and [SEP] tokens if present\n",
        "            for j, seq in enumerate(batch_seqs):\n",
        "                seq_len = len(seq.replace(\" \", \"\"))\n",
        "                # Find where the actual window ends (excluding padding tokens)\n",
        "                emb = outputs.last_hidden_state[j, 1:seq_len+1, :].cpu().numpy()  # skip [CLS], take only window\n",
        "                all_embeddings.append(emb)\n",
        "\n",
        "    return np.stack(all_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ba15f2",
      "metadata": {
        "id": "63ba15f2"
      },
      "outputs": [],
      "source": [
        "def create_sliding_windows(sequence, labels, window_size, stride=1):\n",
        "    \"\"\"Create sliding windows from sequence and corresponding labels\"\"\"\n",
        "    windows = []\n",
        "    window_labels = []\n",
        "    positions = []\n",
        "\n",
        "    # Pad sequence for edge cases\n",
        "    pad_size = window_size // 2 # so starts classification after padding, at first real encoding\n",
        "    padded_seq = 'X' * pad_size + sequence + 'X' * pad_size\n",
        "    padded_labels = [0] * pad_size + labels + [0] * pad_size\n",
        "\n",
        "    # Create sliding windows\n",
        "    for i in range(0, len(sequence), stride):\n",
        "        start_idx = i\n",
        "        end_idx = i + window_size\n",
        "\n",
        "        if end_idx <= len(padded_seq):\n",
        "            window_seq = padded_seq[start_idx:end_idx]\n",
        "            # Label for the center position of the window\n",
        "            center_idx = start_idx + pad_size # residue to predict\n",
        "            if center_idx < len(padded_labels):\n",
        "                center_label = padded_labels[center_idx]\n",
        "\n",
        "                windows.append(window_seq)\n",
        "                window_labels.append(center_label)\n",
        "                positions.append(i)  # Original position in sequence\n",
        "\n",
        "    return windows, window_labels, positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064304e2",
      "metadata": {
        "id": "064304e2"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(fasta_path):\n",
        "    \"\"\"Load FASTA data and preprocess for sliding window approach\"\"\"\n",
        "    records = []\n",
        "\n",
        "    with open(fasta_path, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                        records.append(current_record)\n",
        "\n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\n",
        "                    \"uniprot_ac\": uniprot_ac,\n",
        "                    \"kingdom\": kingdom,\n",
        "                    \"type\": type_,\n",
        "                    \"sequence\": None,\n",
        "                    \"label\": None\n",
        "                }\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "\n",
        "        # Add last record\n",
        "        if current_record is not None:\n",
        "            if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                records.append(current_record)\n",
        "\n",
        "    print(f\"Total records loaded: {len(records)}\")\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_raw = pd.DataFrame(records)\n",
        "\n",
        "    # Filter out sequences with 'P' in labels (if needed)\n",
        "    df = df_raw[~df_raw[\"label\"].str.contains(\"P\")]\n",
        "\n",
        "    # Map signal peptide types to binary classification\n",
        "    df[\"has_signal_peptide\"] = df[\"type\"].map({\n",
        "        \"NO_SP\": 0,\n",
        "        \"LIPO\": 1,\n",
        "        \"SP\": 1,\n",
        "        \"TAT\": 1,\n",
        "        \"TATLIPO\": 1\n",
        "    })\n",
        "\n",
        "    # Balance the dataset at sequence level first\n",
        "    df_majority = df[df[\"has_signal_peptide\"] == 0]\n",
        "    df_minority = df[df[\"has_signal_peptide\"] == 1]\n",
        "\n",
        "    if not df_minority.empty and not df_majority.empty:\n",
        "\n",
        "        n_samples = min(len(df_majority), 5000) # Limit samples to 5000 to prevent high ram usage\n",
        "        df_majority_sampled = resample(\n",
        "            df_majority,\n",
        "            replace=False, # sample without replacement\n",
        "            n_samples=n_samples,\n",
        "            random_state=42\n",
        "        )\n",
        "        df_balanced = pd.concat([df_majority_sampled, df_minority]) # Include all minority samples\n",
        "    else:\n",
        "        df_balanced = df.copy()\n",
        "\n",
        "\n",
        "    # Convert residue-level labels to binary\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "\n",
        "    # Create sliding windows for all sequences\n",
        "    all_windows = []\n",
        "    all_labels = []\n",
        "    all_seq_ids = []\n",
        "\n",
        "    for idx, row in df_balanced.iterrows():\n",
        "        sequence = row[\"sequence\"]\n",
        "        label_string = row[\"label\"]\n",
        "\n",
        "        # Convert label string to binary array\n",
        "        residue_labels = [label_map.get(c, 0) for c in label_string]\n",
        "\n",
        "        # Skip sequences where label length doesn't match sequence length\n",
        "        if len(residue_labels) != len(sequence):\n",
        "            print(\"A sequence length is not equal to the label length\")\n",
        "            continue\n",
        "\n",
        "        # Create sliding windows for this sequence\n",
        "        windows, window_labels, positions = create_sliding_windows(\n",
        "            sequence, residue_labels, WINDOW_SIZE, STRIDE\n",
        "        )\n",
        "\n",
        "        all_windows.extend(windows)\n",
        "        all_labels.extend(window_labels)\n",
        "        all_seq_ids.extend([idx] * len(windows))\n",
        "\n",
        "    print(f\"Total windows created: {len(all_windows)}\")\n",
        "    print(f\"Signal peptide windows: {sum(all_labels)}\")\n",
        "    print(f\"Non-signal peptide windows: {len(all_labels) - sum(all_labels)}\")\n",
        "\n",
        "    return all_windows, all_labels, all_seq_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408cc6b7",
      "metadata": {
        "id": "408cc6b7"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowDataset(Dataset):\n",
        "    def __init__(self, windows, labels, window_size):\n",
        "        self.windows = windows\n",
        "        self.labels = labels\n",
        "        self.window_size = window_size\n",
        "\n",
        "        # Pre-encode all windows\n",
        "        self.encoded_windows = []\n",
        "        for window in windows:\n",
        "            encoded = get_protbert_window_embeddings(window)\n",
        "            self.encoded_windows.append(encoded)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'window': torch.tensor(self.encoded_windows[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c777e161",
      "metadata": {
        "id": "c777e161"
      },
      "outputs": [],
      "source": [
        "class CNNLSTMSignalPeptideClassifier(nn.Module):\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=2,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.lstm_hidden = lstm_hidden\n",
        "        self.lstm_layers = lstm_layers\n",
        "\n",
        "        # CNN layers for local pattern detection\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = num_aa\n",
        "\n",
        "        for out_channels in cnn_channels:\n",
        "            self.conv_layers.append(nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # LSTM layers for sequential dependencies\n",
        "        # Input to LSTM: [batch_size, seq_len, features]\n",
        "        lstm_input_size = cnn_channels[-1]  # Last CNN output channels\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=lstm_input_size,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate LSTM output size\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Attention mechanism to focus on important positions\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, lstm_output_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(lstm_output_size // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        classifier_layers = []\n",
        "        in_dim = lstm_output_size\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "            classifier_layers.extend([\n",
        "                nn.Linear(in_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            in_dim = hidden_dim\n",
        "\n",
        "        # Binary classification output\n",
        "        classifier_layers.append(nn.Linear(hidden_dim, 1))\n",
        "        self.classifier = nn.Sequential(*classifier_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size, seq_len, num_features = x.size()\n",
        "\n",
        "        # need [batch_size, num_aa, window_size] for Conv1d\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "\n",
        "        # Apply CNN layers\n",
        "        for conv_layer in self.conv_layers:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        # need [batch_size, seq_len, features] for LSTM\n",
        "        x = x.transpose(1, 2)  # [batch_size, window_size, cnn_channels[-1]]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, (hidden, cell) = self.lstm(x)\n",
        "        # lstm_out: [batch_size, seq_len, lstm_hidden * directions]\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "\n",
        "        # Weighted sum of LSTM outputs\n",
        "        attended_output = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        # attended_output: [batch_size, lstm_hidden * directions]\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(attended_output)\n",
        "        return logits.squeeze(-1)  # Remove last dimension\n",
        "\n",
        "\n",
        "class CNNLSTMSignalPeptideClassifierV2(nn.Module):\n",
        "    \"\"\"Alternative version with different CNN-LSTM integration\"\"\"\n",
        "    def __init__(self, window_size, num_aa, hidden_dim=128, num_layers=2,\n",
        "                 cnn_channels=[64, 32], lstm_hidden=64, lstm_layers=1,\n",
        "                 use_bidirectional=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_aa = num_aa\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv1d(num_aa, cnn_channels[0], kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(cnn_channels[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv1d(cnn_channels[0], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv1d(cnn_channels[1], cnn_channels[1], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_channels[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # LSTM for sequential modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=cnn_channels[-1],\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=use_bidirectional\n",
        "        )\n",
        "\n",
        "        # Calculate dimensions\n",
        "        lstm_output_size = lstm_hidden * (2 if use_bidirectional else 1)\n",
        "\n",
        "        # Global pooling options\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, window_size, num_aa]\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_aa, window_size]\n",
        "        cnn_features = self.cnn_backbone(x)\n",
        "\n",
        "        # Prepare for LSTM\n",
        "        x = cnn_features.transpose(1, 2)  # [batch_size, window_size, features]\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Global pooling over sequence dimension\n",
        "        lstm_out = lstm_out.transpose(1, 2)  # [batch_size, features, seq_len]\n",
        "        pooled = self.global_pool(lstm_out).squeeze(-1)  # [batch_size, features]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c2361d",
      "metadata": {
        "id": "76c2361d"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs, device,\n",
        "                        lr=0.001, weight_decay=1e-5, patience=5):\n",
        "    \"\"\"Enhanced training function with gradient clipping and better scheduling\"\"\"\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # More sophisticated learning rate scheduling\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=patience, factor=0.5, verbose=True\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                logits = model(windows)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                train_batches += 1\n",
        "\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        if train_batches == 0:\n",
        "            print(\"No successful training batches!\")\n",
        "            break\n",
        "\n",
        "        avg_train_loss = train_loss / train_batches\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                windows = batch['window'].to(device)\n",
        "                labels = batch['label'].to(device).float()\n",
        "\n",
        "                try:\n",
        "                    logits = model(windows)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    val_batches += 1\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(logits) > 0.5).float()\n",
        "                    val_correct += (predictions == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_batches == 0:\n",
        "            print(\"No successful validation batches!\")\n",
        "            break\n",
        "\n",
        "        avg_val_loss = val_loss / val_batches\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping and best model saving\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience * 2:  # More patience for complex model\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6713138b",
      "metadata": {
        "id": "6713138b"
      },
      "outputs": [],
      "source": [
        "# compute percentage of false predicted labels\n",
        "def sequence_level_accuracy(labels, predictions):\n",
        "    \"\"\"Compute the accuracy of individual window predictions.\"\"\"\n",
        "    correct = 0\n",
        "    total = len(labels) # Total number of windows\n",
        "\n",
        "    # Ensure labels and predictions have the same length\n",
        "    if len(labels) != len(predictions):\n",
        "        print(\"Warning: Length of labels and predictions do not match.\")\n",
        "        # Adjust total to the minimum length if lengths differ\n",
        "        total = min(len(labels), len(predictions))\n",
        "        labels = labels[:total]\n",
        "        predictions = predictions[:total]\n",
        "\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Now comparing individual predictions and labels\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfdfb0f",
      "metadata": {
        "id": "0cfdfb0f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate the sliding window model\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            windows = batch['window'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(windows)\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['No Signal', 'Signal']))\n",
        "\n",
        "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    seq_acc = sequence_level_accuracy(all_labels, all_preds)\n",
        "\n",
        "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Sequence-level Accuracy: {seq_acc:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels, all_probs\n",
        "\n",
        "def predict_sequence(model, sequence, window_size, device, threshold=0.5):\n",
        "    \"\"\"Predict signal peptide positions for a full sequence\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create dummy labels (we don't need them for prediction)\n",
        "    dummy_labels = [0] * len(sequence)\n",
        "\n",
        "    # Create sliding windows\n",
        "    windows, _, positions = create_sliding_windows(sequence, dummy_labels, window_size, stride=1)\n",
        "\n",
        "    # Encode windows\n",
        "    encoded_windows = get_protbert_window_embeddings(windows)\n",
        "\n",
        "    predictions = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_window in encoded_windows:\n",
        "            window_tensor = torch.tensor(encoded_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            logit = model(window_tensor)\n",
        "            prob = torch.sigmoid(logit).item()\n",
        "            pred = int(prob > threshold)\n",
        "\n",
        "            predictions.append(pred)\n",
        "            probabilities.append(prob)\n",
        "\n",
        "    return predictions, probabilities, positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8ad5489",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a8ad5489",
        "outputId": "6d6c0dfc-9474-45d7-e045-f5e7f79db249"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data with sliding windows\n",
        "windows, labels, seq_ids = load_and_preprocess_data(FASTA_PATH)\n",
        "\n",
        "# Split data (keeping track of sequence IDs to avoid data leakage)\n",
        "unique_seq_ids = list(set(seq_ids))\n",
        "train_seq_ids, test_seq_ids = train_test_split(unique_seq_ids, test_size=0.2, random_state=42)\n",
        "train_seq_ids, val_seq_ids = train_test_split(train_seq_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create splits based on sequence IDs\n",
        "train_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in train_seq_ids]\n",
        "val_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in val_seq_ids]\n",
        "test_indices = [i for i, seq_id in enumerate(seq_ids) if seq_id in test_seq_ids]\n",
        "\n",
        "# Setup of split of windows and labels\n",
        "train_windows = [windows[i] for i in train_indices]\n",
        "train_labels = [labels[i] for i in train_indices]\n",
        "\n",
        "val_windows = [windows[i] for i in val_indices]\n",
        "val_labels = [labels[i] for i in val_indices]\n",
        "\n",
        "test_windows = [windows[i] for i in test_indices]\n",
        "test_labels = [labels[i] for i in test_indices]\n",
        "\n",
        "print(f\"Train windows: {len(train_windows)}\")\n",
        "print(f\"Validation windows: {len(val_windows)}\")\n",
        "print(f\"Test windows: {len(test_windows)}\")\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = SlidingWindowDataset(train_windows, train_labels, WINDOW_SIZE)\n",
        "val_dataset = SlidingWindowDataset(val_windows, val_labels, WINDOW_SIZE)\n",
        "test_dataset = SlidingWindowDataset(test_windows, test_labels, WINDOW_SIZE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Initialize model (CNN version)\n",
        "model = CNNLSTMSignalPeptideClassifier(\n",
        "    WINDOW_SIZE, 1024, hidden_dim=128, num_layers=2\n",
        ").to(DEVICE)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, EPOCHS, DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e0113a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model for evaluation\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nFinal Evaluation:\")\n",
        "predictions, labels_true, probabilities = evaluate_model(model, test_loader, DEVICE)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Curves')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(probabilities, bins=50, alpha=0.7, label='All Predictions')\n",
        "plt.xlabel('Prediction Probability')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Prediction Probability Distribution')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O0_1O3GUiKEv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "O0_1O3GUiKEv",
        "outputId": "f3e8071d-f7f1-408c-adea-64acf1c38ce7"
      },
      "outputs": [],
      "source": [
        "# Example: Predict on a sample sequence\n",
        "sample_sequence = \"MKKTAIAIAVALAGFATVAQAAPKDNTWYTGAKLGHLQGPVRGVNPTTNAASMKNFTNDIKKEDTSFVTLDAAQ\"\n",
        "print(f\"\\nExample prediction for sequence: {sample_sequence}\")\n",
        "preds, probs, pos = predict_sequence(model, sample_sequence, WINDOW_SIZE, DEVICE)\n",
        "\n",
        "print(\"Position\\tAA\\tProbability\\tPrediction\")\n",
        "for i, (pred, prob, position) in enumerate(zip(preds, probs, pos)):\n",
        "    aa = sample_sequence[position] if position < len(sample_sequence) else 'X'\n",
        "    print(f\"{position:3d}\\t{aa}\\t{prob:.3f}\\t\\t{'Signal' if pred else 'No Signal'}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(f\"\\nFinal model saved to {MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
