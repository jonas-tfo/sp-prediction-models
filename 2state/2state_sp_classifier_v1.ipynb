{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonas-tfo/sp-prediction-models/blob/main/2state/2state_sp_classifier_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac424b32",
      "metadata": {
        "id": "ac424b32"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0e792f",
      "metadata": {
        "id": "fd0e792f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install jax\n",
        "!pip install flax\n",
        "!pip install optax\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "from sklearn.utils import resample\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60e4157",
      "metadata": {
        "id": "c60e4157"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "encoder = AutoModel.from_pretrained(\"Rostlab/prot_bert\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7faf116a",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProteinDataset(Dataset):\n",
        "    \"\"\"Custom dataset for protein sequences\"\"\"\n",
        "    def __init__(self, sequences: List[str], labels: List[List[int]], max_length: int = 512):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Truncate if too long\n",
        "        if len(sequence) > self.max_length:\n",
        "            sequence = sequence[:self.max_length]\n",
        "            label = label[:self.max_length]\n",
        "            \n",
        "        return {\n",
        "            'sequence': sequence,\n",
        "            'label': label,\n",
        "            'length': len(sequence)\n",
        "        }\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"Custom collate function for batching\"\"\"\n",
        "    sequences = [item['sequence'] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "    lengths = [item['length'] for item in batch]\n",
        "    \n",
        "    # Add spaces between amino acids for BERT\n",
        "    spaced_sequences = [\" \".join(seq) for seq in sequences]\n",
        "    \n",
        "    return {\n",
        "        'sequences': spaced_sequences,\n",
        "        'labels': labels,\n",
        "        'lengths': lengths\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3989a185",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientProteinEncoder:\n",
        "    \"\"\"Efficient protein encoder using batched processing\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"Rostlab/prot_bert\", batch_size: int = 16):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        \n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "        self.encoder = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        self.encoder.eval()  # Set to evaluation mode\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def encode_batch(self, sequences: List[str]) -> torch.Tensor:\n",
        "        \"\"\"Encode a batch of sequences efficiently\"\"\"\n",
        "        # Tokenize batch\n",
        "        tokens = self.tokenizer(\n",
        "            sequences,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.encoder(**tokens)\n",
        "            # Get embeddings and remove [CLS] and [SEP] tokens\n",
        "            embeddings = outputs.last_hidden_state[:, 1:-1]  # [batch, seq_len-2, hidden_dim]\n",
        "        \n",
        "        return embeddings\n",
        "    \n",
        "    def encode_dataset(self, dataset: ProteinDataset) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Encode entire dataset efficiently\"\"\"\n",
        "        dataloader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=self.batch_size, \n",
        "            shuffle=False, \n",
        "            collate_fn=collate_batch\n",
        "        )\n",
        "        \n",
        "        all_embeddings = []\n",
        "        all_labels = []\n",
        "        \n",
        "        print(f\"Encoding {len(dataset)} sequences in batches of {self.batch_size}...\")\n",
        "        \n",
        "        for i, batch in enumerate(dataloader):\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Processing batch {i+1}/{len(dataloader)}\")\n",
        "            \n",
        "            # Encode sequences\n",
        "            embeddings = self.encode_batch(batch['sequences'])\n",
        "            \n",
        "            # Process each sequence in the batch\n",
        "            for j, (emb, labels, length) in enumerate(zip(embeddings, batch['labels'], batch['lengths'])):\n",
        "                # Truncate embedding to actual sequence length\n",
        "                emb_truncated = emb[:length].cpu().numpy()\n",
        "                labels_array = np.array(labels[:length])\n",
        "                \n",
        "                all_embeddings.append(emb_truncated)\n",
        "                all_labels.append(labels_array)\n",
        "            \n",
        "            # Clear GPU memory\n",
        "            del embeddings\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        return all_embeddings, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b585d837",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def load_and_prep_data_efficient(data_path: str, max_length: int = 512) -> Tuple[Any, Any, Any, Any]:\n",
        "    \"\"\"Efficiently load and prepare data\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    \n",
        "    records = []\n",
        "    with open(data_path, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] and current_record[\"label\"]:\n",
        "                        # Filter by length\n",
        "                        if len(current_record[\"sequence\"]) <= max_length:\n",
        "                            records.append(current_record)\n",
        "                \n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\n",
        "                    \"uniprot_ac\": uniprot_ac, \n",
        "                    \"kingdom\": kingdom, \n",
        "                    \"type\": type_, \n",
        "                    \"sequence\": None, \n",
        "                    \"label\": None\n",
        "                }\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "        \n",
        "        # Don't forget the last record\n",
        "        if current_record and current_record[\"sequence\"] and current_record[\"label\"]:\n",
        "            if len(current_record[\"sequence\"]) <= max_length:\n",
        "                records.append(current_record)\n",
        "    \n",
        "    print(f\"Loaded {len(records)} valid records\")\n",
        "    \n",
        "    # Create DataFrame and clean data\n",
        "    df = pd.DataFrame(records)\n",
        "    df = df[~df[\"sequence\"].str.contains(\"P\")]  # Remove sequences with 'P'\n",
        "    \n",
        "    # Balance classes\n",
        "    df_majority = df[df[\"type\"] == \"NO_SP\"]\n",
        "    df_minority = df[df[\"type\"] != \"NO_SP\"]\n",
        "    \n",
        "    # Sample down majority class instead of upsampling minority (more efficient)\n",
        "    n_samples = min(len(df_majority), len(df_minority) * 2)  # 2:1 ratio\n",
        "    df_majority_sampled = df_majority.sample(n=n_samples, random_state=42)\n",
        "    df_balanced = pd.concat([df_majority_sampled, df_minority])\n",
        "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Balanced dataset size: {len(df_balanced)}\")\n",
        "    print(\"Class distribution:\")\n",
        "    print(df_balanced[\"type\"].value_counts())\n",
        "    \n",
        "    # Encode labels\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "    df_balanced[\"encoded_label\"] = df_balanced[\"label\"].apply(\n",
        "        lambda x: [label_map[c] for c in x if c in label_map]\n",
        "    )\n",
        "    \n",
        "    # Filter out sequences with no valid labels\n",
        "    df_balanced = df_balanced[df_balanced[\"encoded_label\"].map(len) > 0]\n",
        "    \n",
        "    sequences = df_balanced[\"sequence\"].tolist()\n",
        "    labels = df_balanced[\"encoded_label\"].tolist()\n",
        "    \n",
        "    # Split data first, then encode\n",
        "    train_seqs, test_seqs, train_labels, test_labels = train_test_split(\n",
        "        sequences, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"Training sequences: {len(train_seqs)}\")\n",
        "    print(f\"Test sequences: {len(test_seqs)}\")\n",
        "    \n",
        "    return train_seqs, test_seqs, train_labels, test_labels\n",
        "\n",
        "def pad_sequences(embeddings: List[np.ndarray], labels: List[np.ndarray]) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Pad sequences to same length for JAX\"\"\"\n",
        "    max_len = max(len(emb) for emb in embeddings)\n",
        "    hidden_dim = embeddings[0].shape[1]\n",
        "    \n",
        "    # Pad embeddings\n",
        "    padded_embeddings = np.zeros((len(embeddings), max_len, hidden_dim))\n",
        "    padded_labels = np.zeros((len(labels), max_len))\n",
        "    \n",
        "    for i, (emb, lbl) in enumerate(zip(embeddings, labels)):\n",
        "        padded_embeddings[i, :len(emb)] = emb\n",
        "        padded_labels[i, :len(lbl)] = lbl\n",
        "    \n",
        "    return jnp.array(padded_embeddings), jnp.array(padded_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c5b579",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerResidueClassifier(nn.Module):\n",
        "    \"\"\"Improved classifier with dropout and better architecture\"\"\"\n",
        "    hidden_size: int = 256\n",
        "    dropout_rate: float = 0.1\n",
        "    \n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = True):\n",
        "        x = nn.Dense(self.hidden_size)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
        "        x = nn.Dense(self.hidden_size // 2)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not training)(x)\n",
        "        x = nn.Dense(1)(x)\n",
        "        x = x.squeeze(-1)\n",
        "        return x\n",
        "\n",
        "def create_train_state(model, rng, sample_input, learning_rate=1e-3):\n",
        "    \"\"\"Create training state\"\"\"\n",
        "    params = model.init(rng, sample_input, training=False)\n",
        "    optimizer = optax.adam(learning_rate)\n",
        "    opt_state = optimizer.init(params)\n",
        "    return params, opt_state, optimizer\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, optimizer, model, batch_x, batch_y):\n",
        "    \"\"\"Single training step\"\"\"\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply(params, batch_x, training=True, rngs={'dropout': jax.random.PRNGKey(0)})\n",
        "        loss = optax.sigmoid_binary_cross_entropy(logits, batch_y).mean()\n",
        "        return loss\n",
        "    \n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    \n",
        "    return params, opt_state, loss\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(params, model, batch_x, batch_y):\n",
        "    \"\"\"Single evaluation step\"\"\"\n",
        "    logits = model.apply(params, batch_x, training=False)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(logits, batch_y).mean()\n",
        "    preds = (jax.nn.sigmoid(logits) > 0.5).astype(int)\n",
        "    accuracy = jnp.mean(preds == batch_y)\n",
        "    return loss, accuracy\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    # Set up paths (adjust as needed)\n",
        "    FASTA_PATH = \"/content/drive/MyDrive/PBLRost/data/complete_set_unpartitioned.fasta\"\n",
        "    \n",
        "    # Load and prepare data\n",
        "    train_seqs, test_seqs, train_labels, test_labels = load_and_prep_data_efficient(FASTA_PATH)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_dataset = ProteinDataset(train_seqs, train_labels)\n",
        "    test_dataset = ProteinDataset(test_seqs, test_labels)\n",
        "    \n",
        "    # Encode sequences efficiently\n",
        "    encoder = EfficientProteinEncoder(batch_size=8)  # Smaller batch size for encoding\n",
        "    \n",
        "    print(\"Encoding training data...\")\n",
        "    train_embeddings, train_labels_encoded = encoder.encode_dataset(train_dataset)\n",
        "    \n",
        "    print(\"Encoding test data...\")\n",
        "    test_embeddings, test_labels_encoded = encoder.encode_dataset(test_dataset)\n",
        "    \n",
        "    # Clear encoder to free GPU memory\n",
        "    del encoder\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    # Pad sequences\n",
        "    print(\"Padding sequences...\")\n",
        "    X_train, Y_train = pad_sequences(train_embeddings, train_labels_encoded)\n",
        "    X_test, Y_test = pad_sequences(test_embeddings, test_labels_encoded)\n",
        "    \n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Test data shape: {X_test.shape}\")\n",
        "    \n",
        "    # Initialize model\n",
        "    model = PerResidueClassifier()\n",
        "    rng = jax.random.PRNGKey(42)\n",
        "    \n",
        "    # Create training state\n",
        "    params, opt_state, optimizer = create_train_state(model, rng, X_train[:1])\n",
        "    \n",
        "    # Training parameters\n",
        "    batch_size = 16\n",
        "    num_epochs = 10\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle training data\n",
        "        perm = jax.random.permutation(rng, X_train.shape[0])\n",
        "        X_train_shuffled = X_train[perm]\n",
        "        Y_train_shuffled = Y_train[perm]\n",
        "        \n",
        "        # Training\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for i in range(0, X_train.shape[0], batch_size):\n",
        "            batch_x = X_train_shuffled[i:i + batch_size]\n",
        "            batch_y = Y_train_shuffled[i:i + batch_size]\n",
        "            \n",
        "            params, opt_state, loss = train_step(params, opt_state, optimizer, model, batch_x, batch_y)\n",
        "            epoch_loss += loss\n",
        "            num_batches += 1\n",
        "        \n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        \n",
        "        # Evaluation\n",
        "        eval_loss, eval_acc = eval_step(params, model, X_test, Y_test)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train Loss: {avg_loss:.4f}, \"\n",
        "              f\"Test Loss: {eval_loss:.4f}, \"\n",
        "              f\"Test Acc: {eval_acc:.4f}\")\n",
        "    \n",
        "    return params, model, X_test, Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d064fcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive if in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "except:\n",
        "        pass\n",
        "    \n",
        "    # Train the model\n",
        "    params, model, X_test, Y_test = train_model()\n",
        "    print(\"Training completed!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
