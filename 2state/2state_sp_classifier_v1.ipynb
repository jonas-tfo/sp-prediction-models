{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonas-tfo/sp-prediction-models/blob/main/2state/2state_sp_classifier_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac424b32",
      "metadata": {
        "id": "ac424b32"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0e792f",
      "metadata": {
        "id": "fd0e792f"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install jax\n",
        "!pip install flax\n",
        "!pip install optax\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "from sklearn.utils import resample\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb848355",
      "metadata": {
        "id": "bb848355"
      },
      "source": [
        "## Constants and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60e4157",
      "metadata": {
        "id": "c60e4157"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
        "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
        "encoder = AutoModel.from_pretrained(\"Rostlab/prot_bert\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae8306b",
      "metadata": {
        "id": "aae8306b"
      },
      "outputs": [],
      "source": [
        "def encode_sequence(seq):\n",
        "    seq = \" \".join(seq)  # insert spaces between amino acids\n",
        "    tokens = tokenizer(seq, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        output = encoder(**tokens)\n",
        "        embedding = output.last_hidden_state  # [1, seq_len, 1024]\n",
        "    return embedding[0, 1:-1].cpu().numpy()  # remove [CLS] and [SEP] to match label length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d883ad20",
      "metadata": {
        "id": "d883ad20"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def pad_array(arr, length, pad_value=0):\n",
        "\n",
        "    if len(arr.shape) == 1:\n",
        "        return np.pad(arr, (0, length - len(arr)), constant_values=pad_value)\n",
        "    return np.pad(arr, ((0, length - arr.shape[0]), (0, 0)), constant_values=pad_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "949cad6b",
      "metadata": {
        "id": "949cad6b"
      },
      "outputs": [],
      "source": [
        "def load_and_prep_data(dataPath: str):\n",
        "    import pandas as pd\n",
        "\n",
        "    records = []  # uniprot_ac, kingdom, type_, sequence, label\n",
        "    with open(dataPath, \"r\") as f:\n",
        "        current_record = None\n",
        "        for line in f:\n",
        "            if line.startswith(\">\"):\n",
        "                if current_record is not None:\n",
        "                    if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                        records.append(current_record)\n",
        "                    else:\n",
        "                        print(\"Skipping incomplete record:\", current_record)\n",
        "                uniprot_ac, kingdom, type_ = line[1:].strip().split(\"|\")\n",
        "                current_record = {\"uniprot_ac\": uniprot_ac, \"kingdom\": kingdom, \"type\": type_, \"sequence\": None, \"label\": None}\n",
        "            else:\n",
        "                if current_record[\"sequence\"] is None:\n",
        "                    current_record[\"sequence\"] = line.strip()\n",
        "                elif current_record[\"label\"] is None:\n",
        "                    current_record[\"label\"] = line.strip()\n",
        "                else:\n",
        "                    print(\"Skipping extra line in record:\", current_record)\n",
        "        if current_record is not None:\n",
        "            if current_record[\"sequence\"] is not None and current_record[\"label\"] is not None:\n",
        "                records.append(current_record)\n",
        "            else:\n",
        "                print(\"Skipping incomplete record:\", current_record)\n",
        "\n",
        "    print(f\"Total records: {len(records)}\")\n",
        "    df_raw = pd.DataFrame(records)\n",
        "    df_raw.dropna(subset=['sequence', 'label', 'type'], inplace=True)\n",
        "\n",
        "    # Remove records with 'P' in sequence (if needed)\n",
        "    df = df_raw[~df_raw[\"sequence\"].str.contains(\"P\")].copy()\n",
        "\n",
        "    df_majority = df[df[\"type\"] == \"NO_SP\"]\n",
        "    df_minority = df[df[\"type\"] != \"NO_SP\"]\n",
        "\n",
        "    # Upsample minority class\n",
        "    from sklearn.utils import resample\n",
        "    df_minority_upsampled = resample(df_minority,\n",
        "                                    replace=True,\n",
        "                                    n_samples=len(df_majority),\n",
        "                                    random_state=42)\n",
        "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "    df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
        "    df_encoded = df_upsampled.copy()\n",
        "    df_encoded[\"label\"] = df_encoded[\"label\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
        "    df_encoded = df_encoded[df_encoded[\"label\"].map(len) > 0]\n",
        "\n",
        "    sequences = df_encoded[\"sequence\"].tolist()\n",
        "    labels = df_encoded[\"label\"].tolist()\n",
        "\n",
        "    print(f\"Total records after oversampling: {len(df_encoded)}\")\n",
        "    print(\"Class distribution after oversampling:\")\n",
        "    print(df_encoded[\"type\"].value_counts())\n",
        "\n",
        "    print(\"Encoding...\")\n",
        "\n",
        "    # get embeddings from bert\n",
        "    encoded_seqs = [encode_sequence(seq) for seq in sequences]\n",
        "    encoded_labels = [np.array(lbl) for lbl in labels]\n",
        "\n",
        "    max_len = max(len(seq) for seq in encoded_seqs)\n",
        "    hidden_dim = encoded_seqs[0].shape[1]\n",
        "\n",
        "    X = np.stack([pad_array(seq, max_len) for seq in encoded_seqs])\n",
        "    Y = np.stack([pad_array(lbl, max_len) for lbl in encoded_labels])\n",
        "\n",
        "    X = jnp.array(X)\n",
        "    Y = jnp.array(Y)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_seqs, test_seqs, train_types, test_types = train_test_split(\n",
        "        X, Y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training set size: {len(train_seqs)}\")\n",
        "    print(f\"Test set size: {len(test_seqs)}\")\n",
        "\n",
        "    return train_seqs, test_seqs, train_types, test_types\n",
        "\n",
        "# Usage:\n",
        "train_seqs, test_seqs, train_types, test_types = load_and_prep_data(FASTA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09bc265",
      "metadata": {
        "id": "d09bc265"
      },
      "outputs": [],
      "source": [
        "class PerResidueClassifier(nn.Module):\n",
        "    hidden_size: int = 256\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(self.hidden_size)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(1)(x)\n",
        "        x = x.squeeze(-1)  # shape: [batch, seq_len]\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce03709d",
      "metadata": {
        "id": "ce03709d"
      },
      "outputs": [],
      "source": [
        "# STEP 5: Training loop\n",
        "model = PerResidueClassifier()\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = model.init(rng, train_seqs)\n",
        "optimizer = optax.adam(1e-3)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "@jax.jit\n",
        "def loss_fn(params, X, Y):\n",
        "    logits = model.apply(params, X)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(logits, Y).mean()\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state, X, Y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, X, Y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc2dae76",
      "metadata": {
        "id": "fc2dae76"
      },
      "outputs": [],
      "source": [
        "# STEP 6: Train with mini-batches and evaluate on test set\n",
        "batch_size = 32\n",
        "\n",
        "# Create a simple data loader\n",
        "def data_loader(X, Y, batch_size, shuffle=True):\n",
        "    dataset_size = X.shape[0]\n",
        "    indices = jnp.arange(dataset_size)\n",
        "    if shuffle:\n",
        "        indices = jax.random.permutation(rng, indices)\n",
        "\n",
        "    for i in range(0, dataset_size, batch_size):\n",
        "        batch_indices = indices[i:i + batch_size]\n",
        "        yield X[batch_indices], Y[batch_indices]\n",
        "\n",
        "# Re-initialize parameters and optimizer state for a fresh training run\n",
        "rng = jax.random.PRNGKey(0) # Use a new random key for reproducibility\n",
        "params = model.init(rng, train_seqs[:1]) # Initialize with a small batch\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "\n",
        "num_epochs = 10 # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    for batch_X, batch_Y in data_loader(train_seqs, train_types, batch_size):\n",
        "        params, opt_state, loss = update(params, opt_state, batch_X, batch_Y)\n",
        "        total_loss += loss * batch_X.shape[0]\n",
        "        count += batch_X.shape[0]\n",
        "\n",
        "    avg_loss = total_loss / count\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# STEP 7: Predict and evaluate on the test set\n",
        "test_logits = model.apply(params, test_seqs)\n",
        "test_preds = (jax.nn.sigmoid(test_logits) > 0.5).astype(int)\n",
        "\n",
        "# Simple accuracy calculation (considering padded values might affect this)\n",
        "# A more robust evaluation would involve masking padded values\n",
        "correct_predictions = jnp.sum(test_preds == test_types)\n",
        "total_predictions = test_types.size\n",
        "accuracy = correct_predictions / total_predictions\n",
        "\n",
        "print(\"\\nEvaluation on Test Set:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print predictions for the first sequence in the test set\n",
        "print(\"\\nPredictions for first sequence in test set:\")\n",
        "print(test_preds[0])\n",
        "print(\"Actual labels for first sequence in test set:\")\n",
        "print(test_types[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}