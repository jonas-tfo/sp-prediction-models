{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac424b32",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from sklearn.utils import resample, train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb848355",
   "metadata": {},
   "source": [
    "## Constants and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/PBLRost/\"\n",
    "FASTA_PATH = os.path.join(DRIVE_PATH, \"data/complete_set_unpartitioned.fasta\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "encoder = AutoModel.from_pretrained(\"Rostlab/prot_bert\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(seq):\n",
    "    seq = \" \".join(seq)  # insert spaces between amino acids\n",
    "    tokens = tokenizer(seq, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = encoder(**tokens)\n",
    "        embedding = output.last_hidden_state  # [1, seq_len, 1024]\n",
    "    return embedding[0, 1:-1].cpu().numpy()  # remove [CLS] and [SEP] to match label length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_array(arr, length, pad_value=0):\n",
    "\n",
    "    if len(arr.shape) == 1:\n",
    "        return np.pad(arr, (0, length - len(arr)), constant_values=pad_value)\n",
    "    return np.pad(arr, ((0, length - arr.shape[0]), (0, 0)), constant_values=pad_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949cad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep_data(dataPath: str):\n",
    "    records = []\n",
    "    with open(dataPath, \"r\") as f:\n",
    "        current_record = {}\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_record:\n",
    "                    records.append(current_record)\n",
    "                header = line[1:].strip().split(\"|\")\n",
    "                if len(header) == 3:\n",
    "                    current_record = {\n",
    "                        \"uniprot_ac\": header[0],\n",
    "                        \"kingdom\": header[1],\n",
    "                        \"type\": header[2],\n",
    "                        \"sequence\": \"\"\n",
    "                    }\n",
    "                else:\n",
    "                    current_record = {}\n",
    "            elif current_record:\n",
    "                if not current_record.get(\"sequence\"):\n",
    "                    current_record[\"sequence\"] = line.strip()\n",
    "    if current_record:\n",
    "        records.append(current_record)\n",
    "    df_raw = pd.DataFrame(records)\n",
    "\n",
    "    # drop na rows\n",
    "    df_raw.dropna(subset=['sequence', 'type'], inplace=True)\n",
    "\n",
    "    # Remove records with 'P' in sequence (if needed)\n",
    "    df = df_raw[~df_raw[\"sequence\"].str.contains(\"P\")].copy()\n",
    "\n",
    "    df_majority = df[df[\"type\"] == \"NO_SP\"]\n",
    "    df_minority = df[df[\"type\"] != \"NO_SP\"]\n",
    "\n",
    "    # Upsample minority class\n",
    "    df_minority_upsampled = resample(df_minority,\n",
    "                                    replace=True,     # sample with replacement\n",
    "                                    n_samples=len(df_majority),    # to match majority class\n",
    "                                    random_state=42) # reproducible results\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "    # Shuffle the dataset\n",
    "    df_upsampled = df_upsampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    label_map = {'S': 1, 'T': 1, 'L': 1, 'I': 0, 'M': 0, 'O': 0}\n",
    "\n",
    "    df_encoded = df_upsampled.copy()\n",
    "    df_encoded[\"label\"] = df_encoded[\"label\"].apply(lambda x: [label_map[c] for c in x if c in label_map])\n",
    "    df_encoded = df_encoded[df_encoded[\"label\"].map(len) > 0]  # Remove rows with empty label lists\n",
    "\n",
    "\n",
    "    sequences = df_encoded[\"sequence\"].tolist()\n",
    "    labels = df_encoded[\"label\"].tolist()\n",
    "\n",
    "    df_encoded.describe()\n",
    "\n",
    "    # total records after oversampling\n",
    "    print(f\"Total records after oversampling: {len(df_upsampled)}\")\n",
    "\n",
    "    # majority class distribution\n",
    "    print(\"Class distribution after oversampling:\")\n",
    "    print(df_upsampled[\"type\"].value_counts())\n",
    "\n",
    "    # get embeddings from bert\n",
    "    encoded_seqs = [encode_sequence(seq) for seq in sequences]\n",
    "    encoded_labels = [np.array(lbl) for lbl in labels]\n",
    "\n",
    "    max_len = max(len(seq) for seq in encoded_seqs)\n",
    "    hidden_dim = encoded_seqs[0].shape[1]\n",
    "\n",
    "    # pad the sequences\n",
    "    X = np.stack([pad_array(seq, max_len) for seq in encoded_seqs])\n",
    "    Y = np.stack([pad_array(lbl, max_len) for lbl in encoded_labels])\n",
    "\n",
    "    X = jnp.array(X)  # shape: [batch_size, seq_len, hidden_dim]\n",
    "    Y = jnp.array(Y)  # shape: [batch_size, seq_len]\n",
    "\n",
    "    # each part\n",
    "    train_seqs, test_seqs, train_types, test_types = train_test_split(\n",
    "        X, Y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {len(train_seqs)}\")\n",
    "    print(f\"Test set size: {len(test_seqs)}\")\n",
    "\n",
    "    return train_seqs, test_seqs, train_types, test_types\n",
    "\n",
    "train_seqs, test_seqs, train_types, test_types = load_and_prep_data(FASTA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09bc265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerResidueClassifier(nn.Module):\n",
    "    hidden_size: int = 256\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        x = x.squeeze(-1)  # shape: [batch, seq_len]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce03709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Training loop\n",
    "model = PerResidueClassifier()\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = model.init(rng, train_seqs)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, X, Y):\n",
    "    logits = model.apply(params, X)\n",
    "    loss = optax.sigmoid_binary_cross_entropy(logits, Y).mean()\n",
    "    return loss\n",
    "\n",
    "@jax.jit\n",
    "def update(params, opt_state, X, Y):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, X, Y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Train\n",
    "for epoch in range(10):\n",
    "    params, opt_state, loss = update(params, opt_state, train_seqs, train_types)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Predict on test set\n",
    "logits = model.apply(params, test_seqs)\n",
    "preds = (jax.nn.sigmoid(logits) > 0.5).astype(int)\n",
    "\n",
    "print(\"Predictions for first test sequence:\")\n",
    "print(preds[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
